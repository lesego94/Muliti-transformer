{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# python -m pip install yahoo-finance; pip install yahoo-finance; pip install yfinance --upgrade --no-cache-dir\n",
    "import yfinance as yf\n",
    "# !pip install --upgrade ta; pip install ta\n",
    "import ta as ta\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import t\n",
    "import tensorflow as tf\n",
    "from datetime import date, datetime, timedelta\n",
    "from arch import arch_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data.dropna()\n",
    "\n",
    "#Fitting of GARCH(1,1)\n",
    "def GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GARCH11 = arch_model(AR_Data, dist ='t')\n",
    "    res_GARCH11 = GARCH11.fit(disp='off')\n",
    "    CV_GARCH11 = res_GARCH11.conditional_volatility\n",
    "    For_CV_GARCH11 = np.array(res_GARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11\n",
    "\n",
    "#Fitting of GJR_GARCH(1,1)\n",
    "def GJR_GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GJR_GARCH11 = arch_model(AR_Data, p=1, o=1, q=1, dist ='t')\n",
    "    res_GJR_GARCH11 = GJR_GARCH11.fit(disp='off')\n",
    "    CV_GJR_GARCH11 = res_GJR_GARCH11.conditional_volatility\n",
    "    For_CV_GJR_GARCH11 = np.array(res_GJR_GARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return GJR_GARCH11, res_GJR_GARCH11, CV_GJR_GARCH11, For_CV_GJR_GARCH11\n",
    "\n",
    "#Fitting of TARCH(1,1)\n",
    "def TARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "    res_TARCH11 = TARCH11.fit(disp='off')\n",
    "    CV_TARCH11 = res_TARCH11.conditional_volatility\n",
    "    For_CV_TARCH11 = np.array(res_TARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return TARCH11, res_TARCH11, CV_TARCH11, For_CV_TARCH11\n",
    "\n",
    "#Fitting of EGARCH(1,1)\n",
    "def EGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    EGARCH11 = arch_model(AR_Data, dist ='t', vol=\"EGARCH\")\n",
    "    res_EGARCH11 = EGARCH11.fit(disp='off')\n",
    "    CV_EGARCH11 = res_EGARCH11.conditional_volatility\n",
    "    For_CV_EGARCH11 = np.array(res_EGARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return EGARCH11, res_EGARCH11,CV_EGARCH11, For_CV_EGARCH11\n",
    "\n",
    "#Fitting of Absolute Value GARCH(1,1)\n",
    "def AVGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    AVGARCH11 = arch_model(AR_Data, dist ='t', power=1)\n",
    "    res_AVGARCH11 = AVGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_AVGARCH11 = res_AVGARCH11.conditional_volatility\n",
    "    For_CV_AVGARCH11 = np.array(res_AVGARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return AVGARCH11, res_AVGARCH11, CV_AVGARCH11, For_CV_AVGARCH11\n",
    "\n",
    "#Fitting of FIGARCH11(1,1)\n",
    "def FIGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    FIGARCH11 = arch_model(AR_Data, dist ='t', vol=\"FIGARCH\")\n",
    "    res_FIGARCH11 = FIGARCH11.fit(disp='off')\n",
    "    CV_FIGARCH11 = res_FIGARCH11.conditional_volatility\n",
    "    For_CV_FIGARCH11 = np.array(res_FIGARCH11.forecast(horizon=1).variance.dropna())[0][0]   \n",
    "    return FIGARCH11, res_FIGARCH11, CV_FIGARCH11, For_CV_FIGARCH11\n",
    "\n",
    "#AR models are fitted. As requested by arma package, returns are multiplied by 100 in order to improve the fitting process.\n",
    "#GARCH(1,1), GJR_GARCH(1,1), TARCH(1,1), EGARCH(1,1), AVGARCH(1,1) and FIGARCH(1,1) volatility models are fitted.\n",
    "#T student is assumed as distribution.\n",
    "def AR_Models (Data):\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    return GARCH_Parameters, CV_GARCH, For_CV_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH\n",
    "\n",
    "#MultiHeadSelfAttention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "        \n",
    "#Transformer Keras Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.nb_dict = {}; self.Bagging=5\n",
    "        for i in range(self.Bagging):\n",
    "          self.nb_dict[\"att{0}\".format(i)]=MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    def call(self, inputs, training):\n",
    "        self.att_dict = {}\n",
    "        for i in range(self.Bagging):\n",
    "          self.att_dict[\"att{0}\".format(i)]=self.nb_dict[\"att{0}\".format(i)](tf.keras.layers.Dropout(.1)(inputs))\n",
    "          if i==0: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"att{0}\".format(i)]/self.Bagging \n",
    "          else: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"attn_output\"]+self.att_dict[\"att{0}\".format(i)]/self.Bagging\n",
    "        attn_output = self.dropout1(self.att_dict[\"attn_output\"], training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def Transformer_Model (Shape1, Shape2, HeadsAttention,Dropout, LearningRate):\n",
    "    #Model struture is defined\n",
    "    Input = tf.keras.Input(shape=(Shape1,Shape2), name=\"Input\")\n",
    "    #LSTM is applied on top of the transformer\n",
    "    X = tf.keras.layers.LSTM(units=16, dropout=Dropout, return_sequences=True)(Input)\n",
    "    #Tranformer architecture is implemented\n",
    "    transformer_block_1 = TransformerBlock(embed_dim=16, num_heads=HeadsAttention, ff_dim=8, rate=Dropout)\n",
    "    X = transformer_block_1(X)\n",
    "    #Dense layers are used\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dense(8, activation=tf.nn.sigmoid)(X)\n",
    "    X = tf.keras.layers.Dropout(Dropout)(X)\n",
    "    Output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, name=\"Output\")(X)\n",
    "    model = tf.keras.Model(inputs=Input, outputs=Output)\n",
    "    #Optimizer is defined\n",
    "    Opt = tf.keras.optimizers.legacy.Adam(learning_rate=LearningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    #Model is compiled\n",
    "    model.compile(optimizer=Opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "#It generates the database for fitting transformer. No positional encoding is needed as LSTM plays this role in the model structure\n",
    "def Transformer_Database (Timestep, XData_AR, YData_AR):\n",
    "    Features = XData_AR.shape[1]; Sample = XData_AR.shape[0]-Timestep+1\n",
    "    XDataTrainScaledRNN=np.zeros([Sample, Timestep, Features]); YDataTrainRNN=np.zeros([Sample])\n",
    "    for i in range(Sample):\n",
    "        XDataTrainScaledRNN[i,:,:] = XData_AR[i:(Timestep+i)]\n",
    "        YDataTrainRNN[i] = YData_AR[Timestep+i-1]\n",
    "    return XDataTrainScaledRNN, YDataTrainRNN\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGenerationForecast (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data\n",
    "\n",
    "#Final AR database for forcasting is generated\n",
    "def DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n",
    "    Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "    Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "    XDataForecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'], \n",
    "               'CV_GARCH' : For_CV_GARCH/100, 'CV_GJR_GARCH' : For_CV_GJR_GARCH/100, 'CV_TARCH' : For_CV_TARCH/100, \n",
    "               'CV_EGARCH' : For_CV_EGARCH/100, 'CV_AVGARCH' : For_CV_AVGARCH/100, 'CV_FIGARCH' : For_CV_FIGARCH/100}\n",
    "    return pd.DataFrame([XDataForecast], index=[Index_Forecast]), Data_Forecast['DailyReturns']\n",
    "\n",
    "#Transformed ANN-ARCH model forecast\n",
    "def T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n",
    "    XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n",
    "    XDataForecast = pd.concat([XData_AR,XDataForecast])\n",
    "    XDataForecastTotalScaled = Scaled_Norm.transform(XDataForecast)\n",
    "    XDataForecastTotalScaled_T, Y_T = Transformer_Database(Timestep, XDataForecastTotalScaled, np.zeros(XDataForecastTotalScaled.shape[0]))\n",
    "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T)\n",
    "    return TransformerPrediction[-1][0], XDataForecast.index[-1], TransformerPrediction[0:(XDataForecastTotalScaled_T.shape[0]-1)], ReturnForecast\n",
    "\n",
    "#It calculates VaR taking into consideration the forecasted sigma to calculate the scale parameter\n",
    "def T_ANN_ARCH_VaR (Alpha, HistoricalReturns, ForecastedSigma, DF):\n",
    "    HistoricalMean = np.mean(HistoricalReturns)\n",
    "    ScaleParameter = np.sqrt((ForecastedSigma**2)*((DF-2)/DF))\n",
    "    VaR = -t.ppf(Alpha, DF, loc=HistoricalMean, scale=ScaleParameter)\n",
    "    return VaR\n",
    "\n",
    "#Formula to calculate the VaR of ARCH models\n",
    "def VaR_AR_Model (AR_Model,AR_Model_Results,Alpha):\n",
    "    Cond_Var=AR_Model_Results.forecast(horizon=1).variance.dropna()\n",
    "    Cond_Mean=AR_Model_Results.forecast(horizon=1).mean.dropna()\n",
    "    Quantile_Dist=AR_Model.distribution.ppf([Alpha], AR_Model_Results.params[-1:])\n",
    "    VaR=(-Cond_Mean-np.sqrt(Cond_Var)*Quantile_Dist)/100\n",
    "    return VaR.values\n",
    "\n",
    "#Formula to calculate the VaR of all the ARCH models\n",
    "def VaR_AR_Total(Alpha, GARCH_fit, GJR_GARCH_fit, TARCH_fit, EGARCH_fit, AVGARCH_fit, FIGARCH_fit,GARCH, GJR_GARCH, TARCH, EGARCH, AVGARCH, FIGARCH):\n",
    "    VaR_GARCH = VaR_AR_Model (GARCH,GARCH_fit,Alpha)\n",
    "    VaR_GJR_GARCH = VaR_AR_Model (GJR_GARCH,GJR_GARCH_fit,Alpha)\n",
    "    VaR_TARCH = VaR_AR_Model (TARCH,TARCH_fit,Alpha)\n",
    "    VaR_EGARCH = VaR_AR_Model (EGARCH,EGARCH_fit,Alpha)\n",
    "    VaR_AVGARCH = VaR_AR_Model (AVGARCH,AVGARCH_fit,Alpha)\n",
    "    VaR_FIGARCH = VaR_AR_Model (FIGARCH,FIGARCH_fit,Alpha)\n",
    "    return {'VaR_GARCH':VaR_GARCH, 'VaR_GJR_GARCH':VaR_GJR_GARCH, 'VaR_TARCH':VaR_TARCH, 'VaR_EGARCH':VaR_EGARCH, 'VaR_AVGARCH':VaR_AVGARCH, 'VaR_FIGARCH':VaR_FIGARCH}\n",
    "\n",
    "\n",
    "\n",
    "#Fitting of Transformed ANN-ARCH model and forecasting of the next volatility value\n",
    "def T_ANN_ARCH_Fit (Data,Database,Lag=1, LagSD=5, Timestep=10, Dropout=0.05, LearningRate=0.01, Epochs=10000, Alpha=0.005, DF=4, BatchSize=64):\n",
    "    #AR Models are fitted\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    #Database contaning AR models is generated\n",
    "    Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                     CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "    if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "    #Original explanatory and response variables are generated\n",
    "    XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "    #Data is normalized\n",
    "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "    #Data for fitting the transformer model is generated\n",
    "    XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "    #Model with transformer layer is defined\n",
    "    model = Transformer_Model(XData_AR_Norm_T.shape[1], XData_AR_Norm_T.shape[2], HeadsAttention=4, Dropout=Dropout, LearningRate=LearningRate)\n",
    "    model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, verbose=0, batch_size=BatchSize); tf.keras.backend.clear_session()\n",
    "    Forecast, Date_Forecast, TrainPrediction, ReturnForecast = T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n",
    "    VaR = T_ANN_ARCH_VaR(Alpha, Data['DailyReturnsOld'], Forecast,DF)\n",
    "    return {'T_ANN_ARCH_model':model, 'Forecast_T_ANN_ARCH':Forecast, 'Date_Forecast':Date_Forecast, 'TrainPrediction': TrainPrediction, 'Scaler':Scaled_Norm, 'Forecast_GARCH':For_CV_GARCH, 'Forecast_GJR_GARCH':For_CV_GJR_GARCH, 'Forecast_TARCH':For_CV_TARCH, 'Forecast_EGARCH':For_CV_EGARCH, 'Forecast_AVGARCH':For_CV_AVGARCH, 'Forecast_FIGARCH':For_CV_FIGARCH, 'ReturnForecast':ReturnForecast, 'GARCH_fit': GARCH_Parameters, 'GJR_GARCH_fit':GJR_GARCH_Parameters, 'TARCH_fit':TARCH_Parameters, 'EGARCH_fit':EGARCH_Parameters, 'AVGARCH_fit':AVGARCH_Parameters, 'FIGARCH_fit':FIGARCH_Parameters, 'GARCH': GARCH, 'GJR_GARCH':GJR_GARCH, 'TARCH':TARCH, 'EGARCH':EGARCH, 'AVGARCH':AVGARCH, 'FIGARCH':FIGARCH, 'YData_Train':YData_AR_Norm_T, 'VaR': VaR}\n",
    "#     return (model, Forecast, Date_Forecast, TrainPrediction, Scaled_Norm, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH, ReturnForecast, GARCH_Parameters, GJR_GARCH_Parameters, TARCH_Parameters, EGARCH_Parameters, AVGARCH_Parameters, FIGARCH_Parameters, GARCH, GJR_GARCH, TARCH, EGARCH, AVGARCH, FIGARCH, YData_AR_Norm_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.__future__ import reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2009-01-02', '2009-01-09', '2009-01-16', '2009-01-23',\n",
       "               '2009-01-30', '2009-02-06', '2009-02-13', '2009-02-20',\n",
       "               '2009-02-27', '2009-03-06',\n",
       "               ...\n",
       "               '2022-10-28', '2022-11-04', '2022-11-11', '2022-11-18',\n",
       "               '2022-11-25', '2022-12-02', '2022-12-09', '2022-12-16',\n",
       "               '2022-12-23', '2022-12-30'],\n",
       "              dtype='datetime64[ns]', name='Date', length=731, freq='W-FRI')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2009-01-01'; End='2022-12-31'; \n",
    "asset = \"^GSPC\"\n",
    "asset_name = re.sub('[\\W\\d_]+', '', asset)\n",
    "IndexEndDays=yf.download(asset,start=Start,  end=End, progress=False).resample('W-FRI').last().index\n",
    "\n",
    "IndexEndDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "asset = \"^GSPC\"\n",
    "asset_name = re.sub('[\\W\\d_]+', '', asset)\n",
    "IndexEndDays=yf.download(asset,start=Start,  end=End, progress=False).resample('W-FRI').last().index\n",
    "\n",
    "Lag=1; LagSD=4; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100; Alpha=0.005; DF=4\n",
    "\n",
    "DataValidation = DatabaseGeneration(yf.download(asset,start='2000-01-01', end=date.today()+timedelta(days=1), progress=False).resample('W-FRI').last(), Lag, LagSD)\n",
    "\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(asset,start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False).resample('W-FRI').last()\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data,Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "\n",
    "    ResultsCollection.loc[len(ResultsCollection)] = IterResults\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv(f'./output/5_MTL_GARCH_{asset_name}.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Logic\n",
    "\n",
    "1. Start at a windows length from start variable.\n",
    "2. Predict one step ahead, (dont move yet)\n",
    "3. Append the prediction, now we have winodw +1 amount of data, \n",
    "4. do this 3 more times. \n",
    "5. Store the 4th prediction away in a list that has month end variances. \n",
    "6. Then move to a whole month\n",
    "\n",
    "so level 0 loop, is 1 month\n",
    "    level 1 loop is in range of (1,2) t+1\n",
    "\n",
    "so lets start by creating a monthly index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2008-01-31', '2008-02-29', '2008-03-31', '2008-04-30',\n",
       "               '2008-05-31', '2008-06-30', '2008-07-31', '2008-08-31',\n",
       "               '2008-09-30', '2008-10-31', '2008-11-30', '2008-12-31',\n",
       "               '2009-01-31', '2009-02-28', '2009-03-31', '2009-04-30',\n",
       "               '2009-05-31', '2009-06-30', '2009-07-31', '2009-08-31',\n",
       "               '2009-09-30', '2009-10-31', '2009-11-30', '2009-12-31',\n",
       "               '2010-01-31', '2010-02-28', '2010-03-31', '2010-04-30',\n",
       "               '2010-05-31', '2010-06-30', '2010-07-31', '2010-08-31',\n",
       "               '2010-09-30', '2010-10-31', '2010-11-30', '2010-12-31',\n",
       "               '2011-01-31', '2011-02-28', '2011-03-31', '2011-04-30',\n",
       "               '2011-05-31', '2011-06-30', '2011-07-31', '2011-08-31',\n",
       "               '2011-09-30', '2011-10-31', '2011-11-30', '2011-12-31',\n",
       "               '2012-01-31', '2012-02-29', '2012-03-31', '2012-04-30',\n",
       "               '2012-05-31', '2012-06-30', '2012-07-31', '2012-08-31',\n",
       "               '2012-09-30', '2012-10-31', '2012-11-30', '2012-12-31',\n",
       "               '2013-01-31', '2013-02-28', '2013-03-31', '2013-04-30',\n",
       "               '2013-05-31', '2013-06-30', '2013-07-31', '2013-08-31',\n",
       "               '2013-09-30', '2013-10-31', '2013-11-30', '2013-12-31',\n",
       "               '2014-01-31', '2014-02-28', '2014-03-31', '2014-04-30',\n",
       "               '2014-05-31', '2014-06-30', '2014-07-31', '2014-08-31',\n",
       "               '2014-09-30', '2014-10-31', '2014-11-30', '2014-12-31',\n",
       "               '2015-01-31', '2015-02-28', '2015-03-31', '2015-04-30',\n",
       "               '2015-05-31', '2015-06-30', '2015-07-31', '2015-08-31',\n",
       "               '2015-09-30', '2015-10-31', '2015-11-30', '2015-12-31'],\n",
       "              dtype='datetime64[ns]', name='Date', freq='M')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "asset = \"^GSPC\"\n",
    "asset_name = re.sub('[\\W\\d_]+', '', asset)\n",
    "IndexEndMonths=yf.download(asset,start= pd.to_datetime(Start) ,  end=End, progress=False).resample('M').last().index\n",
    "IndexEndMonths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database=yf.download(asset,start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False).resample('W-FRI').last()\n",
    "Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-03-24</th>\n",
       "      <td>1301.670044</td>\n",
       "      <td>1306.530029</td>\n",
       "      <td>1298.890015</td>\n",
       "      <td>1302.949951</td>\n",
       "      <td>1302.949951</td>\n",
       "      <td>2326070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-31</th>\n",
       "      <td>1300.250000</td>\n",
       "      <td>1303.000000</td>\n",
       "      <td>1294.869995</td>\n",
       "      <td>1294.869995</td>\n",
       "      <td>1294.869995</td>\n",
       "      <td>2236710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-04-07</th>\n",
       "      <td>1309.040039</td>\n",
       "      <td>1314.069946</td>\n",
       "      <td>1294.180054</td>\n",
       "      <td>1295.500000</td>\n",
       "      <td>1295.500000</td>\n",
       "      <td>2082470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-04-14</th>\n",
       "      <td>1288.119995</td>\n",
       "      <td>1292.089966</td>\n",
       "      <td>1283.369995</td>\n",
       "      <td>1289.119995</td>\n",
       "      <td>1289.119995</td>\n",
       "      <td>1891940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-04-21</th>\n",
       "      <td>1311.459961</td>\n",
       "      <td>1317.670044</td>\n",
       "      <td>1306.589966</td>\n",
       "      <td>1311.280029</td>\n",
       "      <td>1311.280029</td>\n",
       "      <td>2392630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-23</th>\n",
       "      <td>822.159973</td>\n",
       "      <td>838.609985</td>\n",
       "      <td>806.070007</td>\n",
       "      <td>831.950012</td>\n",
       "      <td>831.950012</td>\n",
       "      <td>5832160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-30</th>\n",
       "      <td>845.690002</td>\n",
       "      <td>851.659973</td>\n",
       "      <td>821.669983</td>\n",
       "      <td>825.880005</td>\n",
       "      <td>825.880005</td>\n",
       "      <td>5350580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-06</th>\n",
       "      <td>846.090027</td>\n",
       "      <td>870.750000</td>\n",
       "      <td>845.419983</td>\n",
       "      <td>868.599976</td>\n",
       "      <td>868.599976</td>\n",
       "      <td>6484100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-13</th>\n",
       "      <td>833.950012</td>\n",
       "      <td>839.429993</td>\n",
       "      <td>825.210022</td>\n",
       "      <td>826.840027</td>\n",
       "      <td>826.840027</td>\n",
       "      <td>5296650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-20</th>\n",
       "      <td>787.909973</td>\n",
       "      <td>797.580017</td>\n",
       "      <td>777.030029</td>\n",
       "      <td>778.940002</td>\n",
       "      <td>778.940002</td>\n",
       "      <td>5746940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2006-03-24  1301.670044  1306.530029  1298.890015  1302.949951  1302.949951   \n",
       "2006-03-31  1300.250000  1303.000000  1294.869995  1294.869995  1294.869995   \n",
       "2006-04-07  1309.040039  1314.069946  1294.180054  1295.500000  1295.500000   \n",
       "2006-04-14  1288.119995  1292.089966  1283.369995  1289.119995  1289.119995   \n",
       "2006-04-21  1311.459961  1317.670044  1306.589966  1311.280029  1311.280029   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2009-01-23   822.159973   838.609985   806.070007   831.950012   831.950012   \n",
       "2009-01-30   845.690002   851.659973   821.669983   825.880005   825.880005   \n",
       "2009-02-06   846.090027   870.750000   845.419983   868.599976   868.599976   \n",
       "2009-02-13   833.950012   839.429993   825.210022   826.840027   826.840027   \n",
       "2009-02-20   787.909973   797.580017   777.030029   778.940002   778.940002   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2006-03-24  2326070000  \n",
       "2006-03-31  2236710000  \n",
       "2006-04-07  2082470000  \n",
       "2006-04-14  1891940000  \n",
       "2006-04-21  2392630000  \n",
       "...                ...  \n",
       "2009-01-23  5832160000  \n",
       "2009-01-30  5350580000  \n",
       "2009-02-06  6484100000  \n",
       "2009-02-13  5296650000  \n",
       "2009-02-20  5746940000  \n",
       "\n",
       "[153 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Database=yf.download(asset,start=pd.to_datetime(Start)-timedelta(days=650), end=IndexEndDays[i].date(), progress=False).resample('W-FRI').last()\n",
    "Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Database.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for month in IndexEndMonths: \n",
    "    for week in range(1,5):\n",
    "        #we need to predict the next weeks variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag\n",
    "    Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
