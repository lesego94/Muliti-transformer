{"cells":[{"metadata":{"_uuid":"6a13c061ad5779bb65aca7785bb293d1b2868192"},"cell_type":"markdown","source":"#### ------------------------------------------------------------ WORK IN PROGRESS ------------------------------------------------------------------------------------------\n# Credits\nSome space credits to [sources](https://stackexchange.com/). \n\n### Authors: Fernando Lasso\n### Project: Seminar Case Studies ESE Bachelor Econometrics\n### Project members: TBA\n\n# Abstract\n\nThis code has been written for a case study exploring the research question \"Do volatility modelled portfolios ourperform the naive strategy\".\n\n# Objective:\n\nA notebook providing the code used to calculate the returns of a monthly rebalanced portfolio.\n\n**Table of content:**\n\n1. [Libraries and data](#1-bullet)\n2. [Data enrichment](#2-bullet)\n3. [Portfolio creation and evaluation](#3-bullet)\n4. [Benchmark strategies](#4-bullet)\n5. [GARCH Theory](#5-bullet)\n6. [GARCH Performance](#6-bullet)\n7. [Strategies over time](#7-bullet)\n"},{"metadata":{"trusted":true,"_uuid":"4337a427e6e36ed8097e88825202144a90accf9c"},"cell_type":"markdown","source":" # 1. Library and data:<a class=\"anchor\" id=\"1-bullet\"></a>\n \n Below, a list of libraries necessary for the calculations is loaded."},{"metadata":{"_uuid":"897cadc363915b032de11bff8edec0faf20e8bf4","_execution_state":"idle","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Read all libraries\npackages = c(\n             \"PerformanceAnalytics\",\n             #\"e1071\" ,\n             #\"nlme\",\n             #\"SpatioTemporal\",\n             \"readxl\",\n             #\"timeSeries\",\n             #\"fGarch\",\n             \"dplyr\",\n             #\"zoo\",\n             #\"assert_that\",\n             \"ggplot2\",\n             \"reshape2\",\n             #\"quantmod\",\n             \"quadprog\",\n             \"lubridate\",\n             \"foreach\",\n             #\"caret\",\n             #\"car\",\n             \"Matrix\",\n             \"logOfGamma\",\n             \"tseries\") \n\n\nfor (m in 1:length(packages)){\n  cat(\"Loading package: \",packages[m],\"\\n\")\n  suppressMessages(library(packages[m],character.only=TRUE))\n}\n\ncat(\"... finished loading packages \\n\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"540023de64cec0cfb54912ababa98fc9ac517111"},"cell_type":"markdown","source":"The data consists of daily simple returns of five factors over 657 months, from July 1963 until March 2018.\nThese are based on the [Fama and French](https://www.jstor.org/stable/2329302) Three Factor Model and the more recently their [Five Factor Model](https://www.sciencedirect.com/science/article/pii/S0304405X15000616). The factors used in the Three Factor Model are: the market factor in excess over the risk-free return ($\\text{MKT} - r_f$), a size factor SMB (small minus big) and a value factor HML (high minus low). The size factor is constructed by taking a long position in small stocks and short position in big stocks, while the value factor HML encompasses the difference between returns of high and low book-to-market ratio firms.\n\nCriticism on exclusion of profitability and investment based portfolios led to the construction of the Five Factor Model, which extends the Three factor model by including a momentum profitability factor RMW (robust minus weak) and investment factor CMA (conservative minus aggressive). Except for the market factor, all factors are zero-investment portfolios."},{"metadata":{"trusted":true,"_uuid":"4d88024916c20bf42e3d52010fc6b9d7c15ff557","_kg_hide-input":true},"cell_type":"code","source":"# Read excel\ndata <- read_excel(\"../input/famadata/F-F_Research_Data_5_Factors_2x3_daily.xlsx\",skip = 2)\nrecessions <- read_excel(\"../input/famadata/Recessions.xlsx\",skip = 10)\nprint(summary(data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66bef8b786d0951ec10de8b453eb5a5b4372f434"},"cell_type":"markdown","source":"As you can see, the information on returns runs from half 1963 till the beginning of 2018. The returns are expressed as daily percentages and corrected for the risk free rate. Note that a mean return of 0.025 means that on average, 0.025 percent return is expected per day when investing in this portfolio. The cumulative return for one portfolio over all of the 54 years would thus be the product of its rows. For the market porfolio, this means that if you would have invested one dollar in 1963, it would now be worth:"},{"metadata":{"trusted":true,"_uuid":"ce13b0b2d59c52e9b9f08c27461bb0621e48cefe","_kg_hide-input":true},"cell_type":"code","source":"prod(1+ data$`Mkt-RF`/100 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd95e0f7d1a09b978cfef1e78a8a2c33311cf270"},"cell_type":"markdown","source":" # 2. Data Enrichment:<a class=\"anchor\" id=\"2-bullet\"></a>\n \n Next, some feature engineering from the available information. Also, only a subset from the data will be taken to speed up the run time."},{"metadata":{"trusted":true,"_uuid":"25b673366bdefb4e85070e2247f622bec5c73ea8"},"cell_type":"code","source":"x <- 0.3","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"eba389ccadbd3397eb5bdcb1e71601b02e854703"},"cell_type":"code","source":"#Take last x percent of data\ndata <- tail(data, nrow(data)*x)\n\n# Correct the date format\ndata$Date <-  as.Date(as.character(data$X__1), format=\"%Y%m%d\")\n\n# Get year \ndata$year <- year(data$Date)\n\n# Get month\ndata$month <- month(data$Date)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150adabe279b4ba959a56733c3955480aa8d55eb"},"cell_type":"markdown","source":"From the Recessions data, we will add a column to our dataframe containing a dummy for recessions"},{"metadata":{"trusted":true,"_uuid":"0f5565864324f11dc20193b1797e85ec013316d1","_kg_hide-input":true},"cell_type":"code","source":"# Get Recession dummy\nrecessions$year <- substr(recessions[[1]],1,4)\nrecessions$month <- substr(recessions[[1]],6,7)\nrecessions$month[substr(recessions$month,1,1)==0] <- substr(recessions$month[substr(recessions$month,1,1)==0] ,2,2)\n\ndata <- merge(data,recessions, by = c(\"year\",\"month\"),all.x=T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96dd50750528fe03c8b8476c9e443862eff7c346"},"cell_type":"markdown","source":"Since our portfolio strategies will choose from these factors, we will create a new dataframe to pass as an input to our functions."},{"metadata":{"trusted":true,"_uuid":"4986668fe380cd4702911e1220b5414b838eef86","_kg_hide-input":false},"cell_type":"code","source":"inputData <- data[,c(\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\")]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69839d5cda17d4045943aac2631c9cfe20c7307d"},"cell_type":"markdown","source":"# 3. Portfolio creation and evaluation: <a class=\"anchor\" id=\"3-bullet\"></a>\n\nIn order to re-balance the portfolio at the end of every month, it is necessary to evaluate the performance of the individual factors throughout the month. At the start of each month, wealth is either allocated equally $1/N$ across the factors, inversely proportional to the previous month’s volatility (managed volatility) or inversely proportional to the forecasted volatility (modelled volatility). After each subsequent day, the current weight of each factor $i$ in the portfolio is re-calculated based on the returns of the previous day:\n\n![ImgurF1](https://i.imgur.com/gz6VPaV.png)\n\nwhere $m$ denotes the number of days in the current month. In **(1)**, dividing by the term  $\\sum_{j=1}^5 w_{t+d-1}^{(j)} (1 + r_{t+d-1}^{(j)})$ ensures that the weights assigned to each factor sum to one at all times. Then, the daily return of the entire portfolio is found by taking a weighted average of the individual factor returns, where weights are computed as in **(1)**. Finally, the portfolio is re-balanced at the end of the month based on the end-of-month wealth and choice of weighting rule used. The end-of-month wealth $W_{t+m}^{(P)}$ is determined by multiplying the previous month’s wealth $W_t^{(P)}$ by the cumulative portfolio return over the entire month: \n\nClick on the *code* button below to see the function that will loop through our portfolio returns and rebalance the chosen weights. \n"},{"metadata":{"trusted":true,"_uuid":"768c197cd65c27e6f4f9531ad7a2c9a24d1ff97a","_kg_hide-input":true},"cell_type":"code","source":"# Evaluate performance of a strategy function on a portfolio given the rebalancing frequency\nportfolio_returns <- function(portfolio, order.by, strategy, window = NULL, target = NULL){\n    # Get number of rows, set starting weight and returns\n    n <- nrow(portfolio)\n    weights <- rep(1,m)\n    weights <- weights/sum(weights)\n    returns <- rep(0,n)\n\n\n    # Progress bar because why not\n    pb <- txtProgressBar(title = \"progress bar\", min = 0,\n                       max = n, width = 100)\n\n    # Save first of period\n    first_of_period <- 1\n\n    # Count days in month\n    period <- unlist(data %>% group_by(year,month) %>% mutate(period_n = n()) %>% ungroup() %>% dplyr::select(period_n))\n    \n    # Loop over rows\n    for(t in 1:n){\n        if(t!=1){\n            # Every time a new period begins, rebalance the portfolio weights\n            if(order.by[t]!=order.by[t-1]){\n                # Strategy used, number of assets, give all past returns\n                weights <- getweights(strategy = strategy, returns = head(portfolio,(t-1)), \n                                      last_period_n = (t-first_of_period), next_period_n = period[t],\n                                      window = window, target = target) \n                # Update first of period\n                first_of_period <- 1\n                # Sanity check to make sure weights sum up to one (approximately, allowing for rounding error)\n                if(abs(sum(weights)-1)>1e-05){ print(weights); abortmission}\n            }\n    }\n    \n    # Get todays returns\n    day_returns <-portfolio[t,]\n    \n    # Calculate return from this day from the current weights. Scale weights to 1 and then multiply with percentages.\n    returns[t] <- sum((weights/abs(sum(weights))) * ((day_returns/100)))\n\n    # Recalculate weights\n    weights <-  weights * (1+(day_returns/100))\n    \n    setTxtProgressBar(pb, t, label=paste( round(i/n*100, 0),\"% done\"))\n  }\n  close(pb)\n  return((returns)*100)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2754ab6bd158b6f3d3491cf1786c388ed23ac248"},"cell_type":"markdown","source":"## Performance measures\n\nNeed some metrics to evaluate performance of portfolio. After calculating factor weights and the corresponding returns, performance measures are used to make comparisons between models, both in the full sample and in various sub-samples. Five performance measures are computed: the Sharpe and Sortino ratios,  value-at-risk (VaR), skewness and Jensen's alpha. The annualized Sharpe and Sortino ratios are given by the following formulas: \n\n![F2](https://i.imgur.com/9Hq5IVs.png)\n\nwhere $\\overline{r}=\\sum_{t=1}^{T}r_t$ denotes the average daily return of the portfolio. These measures calculate the risk-adjusted performance of a portfolio and therefore a higher score indicates better performance. The advantage of the Sortino ratio over the more common Sharpe ratio is that volatility associated with large positive returns is not penalized by the measure. VaR, on the other hand,  calculates the maximum loss over a defined period at a 95\\% confidence level: see equation (\\ref{eq:ValueatRisk}). The closer this value is to zero, the better the performance:\n\n![F3](https://i.imgur.com/CZZ1exp.png)\n\nFurthermore, the skewness of the return distribution is assessed. A positive skewness indicates a favourable investment, as gains occur more frequently than losses. It is calculated as the third moment of the empirical distribution. Finally, Jensen's alpha is also considered, which measures the return of a portfolio is excess of the theoretical expected return. \n\nWhen comparing models, one is said to outperform another if its performance measures, in particular the Sortino and Sharpe ratios, are more favorable. "},{"metadata":{"trusted":true,"_uuid":"78a5228cb387697d839db52d8a6f8211ecea2ba1","_kg_hide-input":true},"cell_type":"code","source":"getPerfomances <- function(columns){\n\n    ###### PERFORMANCE MEASURES #######\n    melt.data <- melt(data,id.vars = c(\"year\",\"Mkt-RF\"),measure.vars = columns)\n\n    performances <- melt.data %>% group_by(variable) %>% na.omit() %>% dplyr::summarise(Mean = mean(value), \n                                                                                     STD = sd(value),\n                                                                                     Sharpe = Mean/STD * sqrt(252),\n                                                                                     Sortino = SortinoRatio(value)* sqrt(252),\n                                                                                     VAR = VaR(value/100, method=\"modified\"),\n                                                                                     Skew = skewness(value),\n                                                                                     Jensen = getJensen(value,`Mkt-RF`)\n                                                                                     )    \n    return(performances)\n}\n\n#Calculate Jensen of one column versus the market minus RF\ngetJensen <- function(asset,market){\n    fit <- lm(asset ~ market)\n    return(fit$coefficients[1])\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ebf8e1f174a630de6939c7f6fdaa5c710491bdf"},"cell_type":"markdown","source":"# 4. Benchmark strategies: <a class=\"anchor\" id=\"4-bullet\"></a>"},{"metadata":{"_uuid":"d02ffdc263f259b399c01ca2d412fd00e3b957eb"},"cell_type":"markdown","source":"## Naive portfolio\n\nNaive strategy simply rebalances the investment to be equally spread over N assets.. In other words, it weighs each asset equally 1 over N."},{"metadata":{"trusted":true,"_uuid":"0cb816dc4188e1bc55d5501ce387abb37f6c04c3","_kg_hide-input":true},"cell_type":"code","source":"# Rebalance weights based on underlying portfolio inputs and strategy\ngetweights <- function(strategy, returns, last_period_n, next_period_n, window=NULL, target=NULL){\n\n    # Get number of assets in portfolio\n    m <- ncol(returns)\n    \n    # Naive strategy gives equal weights to all assets\n    if(strategy == \"naive\"){\n        return(rep(1/m, m))\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc1ef14cd6cfa74d4f577ee4906b5c0c55a60fa6"},"cell_type":"markdown","source":"Now, let's calculate the returns of the monthly rebalanced naive portfolio and see a quick summary."},{"metadata":{"trusted":true,"_uuid":"4d732f7c4b95268d5bad6c8fff96cd39829bd8ca","_kg_hide-input":true},"cell_type":"code","source":"# Get returns as percentages\ndata$Naive <- portfolio_returns(inputData,data$month,\"naive\") \nsummary(data$Naive)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"588d75944ec6d72a75f86d9707ecc6f4338cec77"},"cell_type":"markdown","source":"## Volatility Managed portfolio portfolio\n\n Volatility managed strategy allocates weights inversely proportional to the historical variance of the single assets. The forecasted variance for day $t+d$ is calculated as the average volatility of the previous period."},{"metadata":{"trusted":true,"_uuid":"770c2d41f5349200a136906ba580a73408dd6f36","_kg_hide-input":true},"cell_type":"code","source":"# Rebalance weights based on underlying portfolio inputs and strategy\ngetweights <- function(strategy, returns, last_period_n, next_period_n, window=NULL, target=NULL){\n\n    # Get number of assets in portfolio\n    m <- ncol(returns)\n    \n    # Volatility managed strategy gives weights inversely proportional to last month's volatility\n    if(strategy == \"vol\"){\n        lastPeriod <- tail(returns, last_period_n)\n    \n        #Initialize the sigma squared variable, denoting volatility\n        sig2 <- rep(0,last_period_n)\n        \n        # Get last month's mean returns\n        mean_ret <- colMeans(lastPeriod)\n        \n        # See formula\n        sig2 <- colSums(sweep(lastPeriod,2,mean_ret) * sweep(lastPeriod,2,mean_ret)) / last_period_n\n              \n        # Set weights inversely proportional to variance\n        w <- 1/(sig2)\n        \n        # Scale sum of weights to one\n        return(w/abs(sum(w)))\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb6855076458bb19ee22530cbdb9d48e65902c9"},"cell_type":"markdown","source":"Let's observe this portfolio's performance.'"},{"metadata":{"trusted":true,"_uuid":"59169465603d9cfa89f75c00f1d7113dd6aab1d8","_kg_hide-input":true},"cell_type":"code","source":"data$VolMan <- portfolio_returns(inputData,data$month,\"vol\") \nsummary(data$VolMan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2583e3e6f6937fb1776d7d397b779437d8a30144"},"cell_type":"markdown","source":"## Mean Variance Optimized Portfolio\n\nMean-variance optimization takes into account correlations between single factors in order to minimize risk while reaching a certain target return. Hence the following optimization problem is solved: \n\n![F4](https://i.imgur.com/DzcgjQA.png)\n\nwhere $\\mu$ = E[$\\mathbf{R}_{t+1}$], is expected value of the $N$ x 1 vector of risky portfolio returns and  $\\mathbf{\\Sigma_t}$ the estimated covariance matrix conditional on the information available at time $t$. $R_f$ denotes the risk free return, $\\mathbf{1}$ is a vector of ones, $\\mathbf{w_t}$ a vector of weights, and  $\\mu_p$ captures the target return. \n\nOne of the varying factors in this calculation is the number of observations known at time $t$ to include to get an accurate estimate of next months covariance matrix. "},{"metadata":{"trusted":true,"_uuid":"8d69de4521ddedf107e99f9cc56fc5bcd0a62ff4","_kg_hide-input":true},"cell_type":"code","source":"# Window Size\nbigWindow <- 252\nmediumWindow <- 66 \nsmallWindow <-22\n\n# Target returns\nhighTarget <- mean(data$Mkt)\nmidTarget <-  mean(data$`Mkt-RF`)\nlowTarget <-  mean(data$RMW)\n\nMVO_Data <- data[,c(\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2516202d54a4b538822dfbacb659087292908795","_kg_hide-input":true},"cell_type":"code","source":"# Rebalance weights based on underlying portfolio inputs and strategy\ngetweights <- function(strategy, returns, last_period_n, next_period_n, window=NULL, target=NULL){\n\n    # Get number of assets in portfolio\n    m <- ncol(returns)\n    \n    # Minimize variance\n    if(strategy == \"MVarOpt\"){\n        #Initialize weights and leave room for risk free\n        w <- rep(0,m)\n        #Get last t\n        n <-nrow(returns)\n        # Get returns in specified window\n        riskyAssets <- returns[max(n+1-window,1):n,1:(m-1)]\n        # Vector of riskfrees\n        riskFree <- as.numeric(returns[n,m])\n        # Add riskfree to riskyassets\n        riskyAssets <- riskyAssets \n        \n        # Get expected. Maybe use longer window for this??\n        mu <- colMeans(riskyAssets) \n        # Get covariance matrix \n        Dmat <- cov(riskyAssets)\n        # Constant in objective function is 0\n        dvec <- rep(0,m-1)\n        # Constraints:   #1 target rate\n        Amat <- matrix(c(mu,\n                         #2 sum smaller than one\n                         rep(-1,m-1),\n                         # 3 No weight larger than 1\n                         -as.vector(diag(m-1)),\n                         # 4 No weight smaller than 0\n                         as.vector(diag(m-1))\n              ),ncol=2*m)\n\n\n        # Right hand of constraint\n        bvec <- as.matrix(c(target , -1, rep(-1,m-1), rep(0,m-1)))\n\n        # If no solution to optimization program, lower target. If that fails, only invest in risk free\n        solution <- tryCatch(solve.QP(Dmat, t(dvec), Amat, bvec, meq=1, factorized=FALSE)$solution,\n                             error=function(e){\n                               # If no solution available; must lower target! \n                               bvec <- as.matrix(c(mean(mu), 1, rep(-1,n-1), rep(0,n-1)))\n                               # If that also does not work; naive strategy\n                               return(tryCatch(solve.QP(Dmat, t(dvec), Amat, bvec, meq=1, factorized=FALSE)$solution,\n                                               error=function(e) return(rep(0, m-1))))\n                             })\n        # Save solution\n        w[1:(m-1)] <- solution\n        # Weight for risk free asset\n        w[m] <- 1-sum(w[1:(m-1)])\n\n        return(w)\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"698d48fd2cb870f5df1e7deb659c89231cd94775"},"cell_type":"markdown","source":"As an example strategy, let us calculate the returns using covariance matrices constructed from the last month only and a target return equal to the mean return of the Robust minus Weak portfolio."},{"metadata":{"trusted":true,"_uuid":"fbd932cc2bafbe85a29c36d4a184a3fde9de02d7","_kg_hide-input":true},"cell_type":"code","source":"data$MeanVarOpt_SW_LT <-portfolio_returns(MVO_Data,data$month,\"MVarOpt\",window=smallWindow,target=lowTarget) \nsummary(data$MeanVarOpt_SW_LT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3fa17babf052c55f15fa4ad1177b21f02ddbb7d"},"cell_type":"markdown","source":"## Interlude Performances\n\nLet's see how the simple models we have seen so far perform."},{"metadata":{"trusted":true,"_uuid":"af3b5419c27d651bc272929eadb471d0745fc71c","_kg_hide-input":true},"cell_type":"code","source":"models <- c(\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"Naive\",\"VolMan\",\"MeanVarOpt_SW_LT\")\ngetPerfomances(models)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fac00e88ee9b5f00bfbf91e39675e02d71a43e20"},"cell_type":"markdown","source":"# 5. GARCH Theory: <a class=\"anchor\" id=\"5-bullet\"></a>\n\n## Three Specifications\n\n### GARCH(1 , 1)\n\nGeneralized Autoregressive Conditional Heteroskedasticity modelling in its simpelest form fits parameters to a formula that predicts the one-day-ahead variance based on a constant, a mean-reverting factor and an error term. The most common specification, the GARCH(1,1) model, assumes the market shock $\\varepsilon_t = (r_t - \\mu)/\\sigma_t$ to be normally distributed. The modeled variance of each factor at time $t+1$ is calculated using various parameters via the GARCH-filter:\n\n![F5](https://i.imgur.com/P6Ritpk.png)\n\n### GAS Garch\n\nOne assumption made by **(5)** that is very likely to be violated is the normal distribution of its error terms. Taking the square of the error term in its model makes it very susceptible to outlier bias. One proposed solution is the [Generalized Autoregressive Score](http://www.gasmodel.com/background.htm), which replaces our specification as follows:\n\n![F6](https://i.imgur.com/tf0lky4.png)\n\n### Generalized T Distribution\n\nAndrew Harvey and Rutger-Jan Lange proposed another solution to the above stated problem. In their [paper](http://www.econ.cam.ac.uk/research-files/repec/cam/pdf/cwpe1517.pdf), the derive the following GARCH filter:\n\n![F7](https://i.imgur.com/WORDSnw.png)\n\nwhere $K(\\nu,\\gamma) = \\nu^{\\frac{1}{\\gamma}} \\sqrt{\\frac{\\Gamma(\\frac{3}{\\gamma}) \\Gamma(\\frac{\\nu-2}{\\gamma})}{\\Gamma(\\frac{1}{\\gamma})\\Gamma(\\frac{\\nu}{\\gamma})}}, \\quad \\nu>2$.\n\nIt has been derived from the generalized T distribution:\n\n![F8](https://i.imgur.com/Mfkc9do.png)"},{"metadata":{"trusted":true,"_uuid":"8445d5c7a3b8648af4412f7f39009d2f6fd1e846","_kg_hide-input":true},"cell_type":"code","source":"# Garch Filter calculates all sigmas in the previous period given some parameter inputs\nGarchFilter <- function(par,returns, strategy = NULL){\n    # Extract the sample size \n    n <- NROW(returns)\n\n    # Initialize sigmas\n    sig2 <- rep(0,n)\n\n    # Name parameters\n    mu    <- par[1]\n    omega <- par[2]\n    alpha <- par[3]\n    beta  <- par[4]\n    \n    # Unconditional Variance is omega/(1-alpha-beta)\n    uncond2 <- omega/(1-alpha-beta)\n    \n    # First sigma is unconditional variance\n    sig2[1] <- uncond2\n    for(t in 2:n){\n        # Get oneday aheads step by step\n        sig2[t] <- onedayahead(par, lastSig2 = sig2[t-1] , lastReturn = returns[t-1], uncond2 = uncond2, strategy = strategy)\n    }\n    \n  return(unlist(sig2))\n}\n\n# One day head forecast given the parameters, previous observations and the unconditional variance\nonedayahead <- function(par, lastSig2, lastReturn, uncond2, strategy){\n\n    # Name parameters\n    mu    <- par[1]\n    omega <- par[2]\n    alpha <- par[3]\n    beta  <- par[4]\n\n    if(strategy == \"GARCH\"){\n        shock2 <- (lastReturn-mu)^2\n        # Get one day ahead\n        onedayahead <- omega + alpha*shock2 + beta*lastSig2\n    }\n\n    if(strategy == \"GARCH_Gas\"){\n        # Get inverse of degrees of freedom\n        v_1 <- 1/par[5]\n        # Squared error\n        e_t2 <- ((lastReturn-mu)/lastSig2)^2\n        onedayahead <- uncond2 + alpha *  #(1+3*par[5]) * \n          ( (v_1+1)/(v_1+(1-2*v_1) / e_t2 ) - 1) * lastSig2 + (alpha + beta) * (lastSig2 - uncond2)\n    }\n\n    if(strategy == \"GARCH_GenT\"){\n        # Degrees of freedom\n        v <- par[5]\n        #Gamma\n        y <- par[6]\n        # Last days shock\n        shock <- abs((lastReturn-mu) / sqrt(lastSig2))\n        # Get one day ahead\n        onedayahead <- uncond2 + alpha * #(2/par[5]) * (1+(par[5]+1)/par[6]) * \n          ( (1/v +1)/(1/v + (K(v,y)^(-y))/shock) -1) * lastSig2 + (alpha + beta) * (lastSig2-uncond2)\n    }\n    return(onedayahead)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d6e6c8d96b309ddbcde5b4d5f118327d541ff63","_kg_hide-input":true},"cell_type":"markdown","source":"where $\\omega$, $\\alpha$, $\\beta > 0$ ensure non-negative volatility estimates. \n\n\n\n## Maximum Likelihood \n\n### GARCH Parameter Estimates\n\nThe parameters $\\mathbf{\\theta}$ = ($\\omega, \\alpha, \\beta, \\mu$)$'$ are re-estimated by maximizing the following log-likelihood function:\n\n![F9](https://i.imgur.com/QZBWEt4.png)\n\nThis procedure finds the optimal values of the parameters ${\\theta}$ using a rolling window of $T$ days. The length of the rolling window $T$ can be altered depending on the desired time frame considered for forecasting future volatility. \n\n### GAS-GARCH Parameter Estimates\n\nSince the errors are distibuted differently, the log-likelihood function can be derived to now look as follows:\n\n![F10](https://i.imgur.com/dJOuBwo.png)\n\n### E-GARCH Parameter Estimates\n\nSimilarly, the Generalized T Distribution boils the formula down to another maximum likelhood specification:\n\n![F11](https://i.imgur.com/L1tMEWF.png?1)"},{"metadata":{"trusted":true,"_uuid":"6276cfcf0e4ccee14ab5a3e3572813575ab8c368","_kg_hide-input":true},"cell_type":"code","source":"# Function to minimize.\nNegativeLogLikelihood <- function(par,returns, strategy = NULL){\n\n    # Name parameters\n    mu    <- par[1]\n    omega <- par[2]\n    alpha <- par[3]\n    beta  <- par[4]\n    \n    # Calculate sigmas for given parameters\n    sig2 <- GarchFilter(par,returns,strategy)\n    \n    if(strategy == \"GARCH\" | strategy == \"GARCH_Asym\"){\n        LL <- - .5 * log(2*pi) - .5 * log(sig2) - ((returns - mu)^2) / (2*sig2)\n    }\n    \n    if(strategy ==\"GARCH_Gas\"){\n        v <- par[5]\n        LL <- - .5 * log(sig2) + gammaln((v+1)/2) -  .5 * log(v*pi) - gammaln(v/2) - ((v+1)/2)  *   log(1 + ((returns - mu)^2) / (sig2 * v))\n    }\n\n    if(strategy == \"GARCH_GenT\"){\n        v <- par[5]\n        y <- par[6]\n        shock <- abs((returns - mu) / sqrt(sig2))\n        LL <- - .5 * log(sig2) + log(y/(2*(v^(1/y)))) + log(K(v,y)) - (gammaln(1/y) +  gammaln(v/y) - gammaln((v+1)/y)) - \n                ((v+1)/y) * log(1 + (K(v,y)^y) * (shock^y) / v)\n    }\n    \n    # Get negative sum\n    out <- -sum(LL)\n    \n    # In these cases, something invalid happened (negative volatility probably). \n    # To help the optimization algorithm a bit, give it a huuuge penalty\n    if(is.infinite(out) | is.nan(out) | (alpha+beta)>=.99) out <- 9999999999999999999\n\n    return(out)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"196e4b9cfe86d7eded2a3f3e42e6b78307fc329f"},"cell_type":"markdown","source":"### Parameter significance\nFinally, it is also possible to determine the standard errors of our parameter coefficient estimates. Although not further used in this research, it can be used to determine the significance of parameter estimates. The standard errors are approximated by the square root of the diagonals of the inversely approximated finite Hessian."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"72420338a3565de7f8e241bade7cfa1277b3a02d"},"cell_type":"code","source":"# Finite hessian\nfdhess6 <- function(par, returns, fun, strategy){\n  eps <- 1e-08\n  p <- length(par)\n  fx <- do.call(fun,list(par=par, returns=returns,strategy=strategy))\n  \n  # Compute the stepsize (h)\n  h <- (eps^(1/3))*pmax(abs(par),1e-3)\n  xh <- x+h\n  h <- xh-x\n  ee <- sparseMatrix(i=1:p,j=1:p,x=h,dims=c(p,p))\n \n  # Compute forward step \n  g = rep(0,p)\n  foreach(i = 1:p) %dopar% {\n    # Add small increments in all directions\n    g[i] = do.call(fun,list(par=par+ee[,i],returns=returns,strategy=strategy))\n  }\n \n  ht <- matrix(h, ncol = 1)\n  \n  H <- ht %*% h\n  \n  # Compute \"double\" forward step \n  foreach(i = 1:p) %dopar%{\n    foreach(j = i:p) %dopar%{\n      H[i,j] = (do.call(fun,list(par=par+ee[,i]+ee[,j],returns=returns,strategy=strategy))-g[i]-g[j]+fx)/H[i,j]\n      # Mirror\n      H[j,i] = H[i,j]\n    }\n  }\n  \n  return(H)\n}\n\nK <- function(v,y){\n  first <- v^(1/y)\n  top <- gamma(3/y)*gamma((v-2)/y)\n  bottom <- gamma(1/y)*gamma(v/y)\n  return(first * sqrt(top/bottom))\n}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5503f928f33727ac6ae31cdfc24e3c135eeaa2e4"},"cell_type":"markdown","source":"## Demonstration \n\nPutting it all together in code, the process looks as follows. For given returns, optimize the maximum likelihood of possible parameter inputs to get the most likely estimates. The optimization function [optim](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html) allows for lower- and upper-bound limitations on our parameters. The function takes starting values aas input. Note that the starting values given are already tuned to lead to fast convergion. Finally, there is a scaling parameter which allows to differ the step-size estimates per parameters, since some are expected to be optimized over a larger range."},{"metadata":{"trusted":true,"_uuid":"33f93b7716830f11ade0914168baea81dc40f147","_kg_hide-input":true},"cell_type":"code","source":"\n# Calculate garch parameters and possibly their ML approximated standard deviation\ngetParams <- function(returns, strategy, indices = 1:length(returns)){\n    \n    # Take the returns from a particular index range\n    returns <- returns[indices]\n\n    # Set solver starting values\n    if(strategy == \"GARCH\"){\n        # Parameter lower bound and upper bound\n        lowerbound <- c(-99,0,0,0)\n        upperbound <- c(99,5,1,1)\n        # Give starting value to optimization\n        startingvalues <- c(mean(returns),var(returns)/20, .1, 0.85)\n        # Give scale metric for appropriate stepsizes\n        parscale <- c(1,.01,.01,.01)\n    }\n    \n    if(strategy == \"GARCH_Gas\"){\n        # Add v\n        lowerbound <- c(-99,0,0,0,2)\n        upperbound <- c(99,5,1,1,999)\n        startingvalues <- c(mean(returns),var(returns)/20, .1, 0.85, 5)\n        parscale <- c(1,.01,.01,.01,100)\n    }\n    if(strategy == \"GARCH_GenT\"){\n         # Add v\n        lowerbound <- c(-99,0,0,0,2,0)\n        upperbound <- c(99,5,1,1,999,5)\n        startingvalues <- c(mean(returns),var(returns)/20, .1, 0.85, 5, .9)\n        parscale <- c(1,.01,.01,.01,100,.5)\n     }\n\n    # Optimize using optim\n    par <- suppressWarnings( \n                optim(par = startingvalues, fn = NegativeLogLikelihood, returns = returns, \n                      lower = lowerbound, upper = upperbound,strategy=strategy, method=\"L-BFGS-B\",\n                        control = list(parscale=parscale))$par\n            )\n    \n    # Calculate sigmas for given parameters\n    sig2 <- GarchFilter(par = par,returns = returns, strategy = strategy)\n   \n    # Calculate approximated hessian and from that the standard deviations\n    ML_sd <- tryCatch(\n                suppressWarnings( sqrt(abs(diag( solve( \n                        fdhess6(f = \"NegativeLogLikelihood\",par=par,returns = returns,strategy = strategy)))))),\n                        error=function(e){  return(rep(NA, length(par))) })\n\n    return(list(par = par, ML_sd = ML_sd, sigmas = sig2))\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b26533fe5c03190831af3ecd8cce1929e5a3b0b7"},"cell_type":"markdown","source":"As an example, observe the parameter forecasts for the three different GARCH specifications using 300 observations.\n\n##### GARCH Parameters"},{"metadata":{"trusted":true,"_uuid":"951fa907af02df68a947ac63ca956ab6c305b22a","_kg_hide-input":true},"cell_type":"code","source":"params <- getParams(data$`Mkt-RF`,strategy=\"GARCH\",indices = 100:400)\nprint(params$par)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fb867d526840ae803c3698fab40fed39dcda0cf"},"cell_type":"markdown","source":"##### GAS GARCH Parameters"},{"metadata":{"trusted":true,"_uuid":"6ebc724a394cc364b70c08cd60da42f0381d80d6","_kg_hide-input":true},"cell_type":"code","source":"params <- getParams(data$`Mkt-RF`,strategy=\"GARCH_Gas\",indices = 100:400)\nprint(params$par)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09b7f6a296f4345f832e230a0295f427db814fe6"},"cell_type":"markdown","source":"##### Generalized T - GARCH Parameters"},{"metadata":{"trusted":true,"_uuid":"2d35a09759e208f32f882dfe0b9489999198d2a9","_kg_hide-input":true},"cell_type":"code","source":"params <- getParams(data$`Mkt-RF`,strategy=\"GARCH_GenT\",indices = 100:400)\nprint(params$par)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfefe8bd0253e9e6e364bea8b9cff49788371032"},"cell_type":"markdown","source":"# 6. GARCH Performance: <a class=\"anchor\" id=\"6-bullet\"></a>\n\nWhen the parameters for a GARCH specification according to the Maximum Likelihood maximalisation has been found, the strategy is to assign weights inversely proportional to expected variance. GARCH based strategies models volatility rather than using historical data. Using a GARCH specification, the forecasted volatility of each factor over the entire period is the sum of multiple one-day ahead forecasts. Since future shocks are assumed to be normally distributed, the total forecasted volatility over a m-day period would be as follows:\n\n![F12](https://i.imgur.com/k6rBp89.png)"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4dcc8aa2ae2ae387939aa2a90d2e758731ba31a4"},"cell_type":"code","source":"# onedayahead is one day ahead volatility\npredictVol <- function(par, sigmas, lastReturn, next_period_n, strategy = NULL){\n     \n    # Extract parameters\n    mu    <- par[1]\n    omega <- par[2]\n    alpha <- par[3]\n    beta  <- par[4]\n \n    # Last sigma\n    lastSig2 <- sigmas[length(sigmas)] \n    \n    # Calculate unconditional\n    uncond2 <- omega/(1-alpha-beta)\n  \n    # Calculate oneday ahead\n    oneahead <- onedayahead(par = par, lastSig2 = lastSig2, lastReturn = lastReturn, uncond2 = uncond2, strategy = strategy)\n\n    # Formula x\n    pred <- next_period_n * uncond2 + ((1-(alpha+beta)^(next_period_n))/(1-alpha-beta)) * (oneahead-uncond2)\n\n  return(pred)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4afe4a25374ed5e4ba1a7c5f6e45127fbd113371"},"cell_type":"markdown","source":"This gives all the ingredients to provide a new weighting function for our portfolio creation. The weights are set as follows. First, the opimized parameters for the given GARCH specification are calculated. Then, using these parameters, the approximated sigmas, last observed return and days in period to be forecasted, the total expected volatility per asset class is determined. The weights are then set inversely to this modelled volatility forecast."},{"metadata":{"trusted":true,"_uuid":"e8f9d1d9e841bf964a0ca4d1d9acb2e23d760edd","_kg_hide-input":true},"cell_type":"code","source":"# Rebalance weights based on underlying portfolio inputs and strategy\ngetweights <- function(strategy, returns, last_period_n, next_period_n, window=NULL, target=NULL){\n\n    # Get number of assets in portfolio\n    m <- ncol(returns)\n    n <- nrow(returns)\n    #Initialize weights\n    w <- rep(0,n)\n    \n    if(substr(strategy,1,5) == \"GARCH\"){\n        # For each column calculate parameters\n        foreach(i = 1:m) %dopar%{\n            # Calculate parameters\n            parameters <- getParams(returns[[i]], strategy= strategy, indices = max(1,n+1-window):n)\n     \n            # Predict next month's volatility\n            totalVol <- predictVol(par = parameters$par, sigmas = parameters$sigmas, lastReturn = returns[n,i], \n                                   next_period_n = next_period_n, strategy = strategy)\n            \n            #Formula\n            w[i] <- 1/(totalVol)\n        }\n        return(w/abs(sum(w)))\n  }\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6de391cadd0d5080069104f9885684512da571a"},"cell_type":"markdown","source":"Now, let us compute the returns generated from GARCH based rebalancing strategies for our three GARCH specifications. The rolling window is set to 252, which should be about the number of working days in a year.\n\n##### GARCH Strategy"},{"metadata":{"trusted":true,"_uuid":"5554e01f1115cbba2b0e164dca1a0687789ddd06","_kg_hide-input":true},"cell_type":"code","source":"data$Garch252 <- portfolio_returns(inputData,data$month,\"GARCH\", data$ndays, window = 252) \nsummary(data$Garch252)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"820bf0f15e20bc4da55b66f5aa1aa143291103ef"},"cell_type":"markdown","source":"##### GAS GARCH Strategy"},{"metadata":{"trusted":true,"_uuid":"f1f4a871745f941e189727aa0059ed3ab3f805fa","_kg_hide-input":true},"cell_type":"code","source":"data$GasGarch252 <- portfolio_returns(inputData,data$month,\"GARCH_Gas\", data$ndays, window = 252) \nsummary(data$GasGarch252)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e22375582cf37963a8ff560ffa4be0afdbf1033"},"cell_type":"markdown","source":"##### Generalized T - GARCH Strategy"},{"metadata":{"trusted":true,"_uuid":"762b9122702c5fb553ce4d856493a5de94b943b8"},"cell_type":"code","source":"data$GenTGarch252 <- portfolio_returns(inputData,data$month,\"GARCH_GenT\", data$ndays, window = 252) \nsummary(data$GenTGarch252)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ddcee4a1acbd4aa8488cff2ff7579476beb99f7"},"cell_type":"markdown","source":"## FINAL RESULTS"},{"metadata":{"trusted":true,"_uuid":"313140e3f3ae2f6ae8b7d991fb72cdfe12a0816d","_kg_hide-input":true},"cell_type":"code","source":"models <- c(\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"Naive\",\"VolMan\",\"MeanVarOpt_SW_LT\",\"Garch252\",\"GasGarch252\",\"GenTGarch252\")\ngetPerfomances(models)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}