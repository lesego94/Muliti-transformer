{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pypfopt import risk_models\n",
    "from sklearn import preprocessing\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from datetime import date, datetime, timedelta\n",
    "from arch import arch_model\n",
    "from pypfopt import expected_returns\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatabaseGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m IndexEndDays\u001b[39m=\u001b[39myf\u001b[39m.\u001b[39mdownload(asset,start\u001b[39m=\u001b[39mstart,  end\u001b[39m=\u001b[39mend, progress\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mW-FRI\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlast()\u001b[39m.\u001b[39mindex\n\u001b[1;32m      5\u001b[0m Database\u001b[39m=\u001b[39myf\u001b[39m.\u001b[39mdownload(asset,start, end, progress\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mW-FRI\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlast()\n\u001b[0;32m----> 6\u001b[0m Data \u001b[39m=\u001b[39m DatabaseGeneration(Database, Lag, LagSD)\n\u001b[1;32m      7\u001b[0m Data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatabaseGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "start = '2009-01-01';end = '2013-01-01'\n",
    "asset = \"^GSPC\"\n",
    "Lag=1; LagSD=4\n",
    "IndexEndDays=yf.download(asset,start=start,  end=end, progress=False).resample('W-FRI').last().index\n",
    "Database=yf.download(asset,start, end, progress=False).resample('W-FRI').last()\n",
    "Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.__future__ import reindexing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1, modify GARCH models to be multistep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find that the TARCH model does not support a horizon greater than 1, one workaround could be to implement recursive forecasting manually. This would involve using the model to make a one-step ahead forecast, appending that forecast to your time series, and then making the next one-step ahead forecast, and so on until you have made 4 forecasts. However, this approach would also be based on the assumption that future residuals are zero, and it would be computationally more intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data.dropna()\n",
    "\n",
    "#Fitting of GARCH(1,1)\n",
    "def GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GARCH11 = arch_model(AR_Data, dist ='t')\n",
    "    res_GARCH11 = GARCH11.fit(disp='off')\n",
    "    CV_GARCH11 = res_GARCH11.conditional_volatility\n",
    "    For_CV_GARCH11 = np.array(res_GARCH11.forecast(horizon=4).variance.dropna())\n",
    "    return GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11\n",
    "\n",
    "#Fitting of GJR_GARCH(1,1)\n",
    "def GJR_GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GJR_GARCH11 = arch_model(AR_Data, p=1, o=1, q=1, dist ='t')\n",
    "    res_GJR_GARCH11 = GJR_GARCH11.fit(disp='off')\n",
    "    CV_GJR_GARCH11 = res_GJR_GARCH11.conditional_volatility\n",
    "    For_CV_GJR_GARCH11 = np.array(res_GJR_GARCH11.forecast(horizon=4).variance.dropna())\n",
    "    return GJR_GARCH11, res_GJR_GARCH11, CV_GJR_GARCH11, For_CV_GJR_GARCH11\n",
    "\n",
    "#Fitting of TARCH(1,1)\n",
    "def TARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "    res_TARCH11 = TARCH11.fit(disp='off')\n",
    "    CV_TARCH11 = res_TARCH11.conditional_volatility\n",
    "    For_CV_TARCH11 = np.array(res_TARCH11.forecast(horizon=4,method= \"bootstrap\").variance.dropna())\n",
    "    return TARCH11, res_TARCH11, CV_TARCH11, For_CV_TARCH11\n",
    "\n",
    "#Fitting of EGARCH(1,1)\n",
    "def EGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    EGARCH11 = arch_model(AR_Data, dist ='t', vol=\"EGARCH\")\n",
    "    res_EGARCH11 = EGARCH11.fit(disp='off')\n",
    "    CV_EGARCH11 = res_EGARCH11.conditional_volatility\n",
    "    For_CV_EGARCH11 = np.array(res_EGARCH11.forecast(horizon=4,method=\"bootstrap\").variance.dropna())\n",
    "    return EGARCH11, res_EGARCH11,CV_EGARCH11, For_CV_EGARCH11\n",
    "\n",
    "#Fitting of Absolute Value GARCH(1,1)\n",
    "def AVGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    AVGARCH11 = arch_model(AR_Data, dist ='t', power=1)\n",
    "    res_AVGARCH11 = AVGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_AVGARCH11 = res_AVGARCH11.conditional_volatility\n",
    "    For_CV_AVGARCH11 = np.array(res_AVGARCH11.forecast(horizon=4,method=\"bootstrap\").variance.dropna())\n",
    "    return AVGARCH11, res_AVGARCH11, CV_AVGARCH11, For_CV_AVGARCH11\n",
    "\n",
    "#Fitting of FIGARCH11(1,1)\n",
    "def FIGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    FIGARCH11 = arch_model(AR_Data, dist ='t', vol=\"FIGARCH\")\n",
    "    res_FIGARCH11 = FIGARCH11.fit(disp='off')\n",
    "    CV_FIGARCH11 = res_FIGARCH11.conditional_volatility\n",
    "    For_CV_FIGARCH11 = np.array(res_FIGARCH11.forecast(horizon=4,method=\"bootstrap\").variance.dropna())\n",
    "    return FIGARCH11, res_FIGARCH11, CV_FIGARCH11, For_CV_FIGARCH11\n",
    "\n",
    "\n",
    "def Transformer_Database (Timestep, XData_AR, YData_AR):\n",
    "    Features = XData_AR.shape[1]\n",
    "    Sample = XData_AR.shape[0] - Timestep - 3  # Adjusted to allow for a 4-step-ahead target\n",
    "    XDataTrainScaledRNN = np.zeros([Sample, Timestep, Features])\n",
    "    YDataTrainRNN = np.zeros([Sample, 4])  # Adjusted for 4-step-ahead forecasts\n",
    "    \n",
    "    for i in range(Sample):\n",
    "        XDataTrainScaledRNN[i,:,:] = XData_AR[i:(Timestep+i)]\n",
    "        YDataTrainRNN[i, :] = YData_AR[(Timestep+i):(Timestep+i+4)]  # 4-step-ahead target\n",
    "    \n",
    "    return XDataTrainScaledRNN, YDataTrainRNN\n",
    "\n",
    "#MultiHeadSelfAttention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "        \n",
    "#Transformer Keras Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.nb_dict = {}; self.Bagging=5\n",
    "        for i in range(self.Bagging):\n",
    "          self.nb_dict[\"att{0}\".format(i)]=MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    def call(self, inputs, training):\n",
    "        self.att_dict = {}\n",
    "        for i in range(self.Bagging):\n",
    "          self.att_dict[\"att{0}\".format(i)]=self.nb_dict[\"att{0}\".format(i)](tf.keras.layers.Dropout(.1)(inputs))\n",
    "          if i==0: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"att{0}\".format(i)]/self.Bagging \n",
    "          else: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"attn_output\"]+self.att_dict[\"att{0}\".format(i)]/self.Bagging\n",
    "        attn_output = self.dropout1(self.att_dict[\"attn_output\"], training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "#Database is calculated\n",
    "def DatabaseGenerationForecast (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data\n",
    "\n",
    "def Transformer_Model (Shape1, Shape2, HeadsAttention,Dropout, LearningRate):\n",
    "    #Model struture is defined\n",
    "    Input = tf.keras.Input(shape=(Shape1,Shape2), name=\"Input\")\n",
    "    #LSTM is applied on top of the transformer\n",
    "    X = tf.keras.layers.LSTM(units=16, dropout=Dropout, return_sequences=True)(Input)\n",
    "    #Tranformer architecture is implemented\n",
    "    transformer_block_1 = TransformerBlock(embed_dim=16, num_heads=HeadsAttention, ff_dim=8, rate=Dropout)\n",
    "    X = transformer_block_1(X)\n",
    "    #Dense layers are used\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dense(8, activation=tf.nn.sigmoid)(X)\n",
    "    X = tf.keras.layers.Dropout(Dropout)(X)\n",
    "    Output = tf.keras.layers.Dense(4, activation=tf.nn.sigmoid, name=\"Output\")(X)\n",
    "    model = tf.keras.Model(inputs=Input, outputs=Output)\n",
    "    #Optimizer is defined\n",
    "    Opt = tf.keras.optimizers.legacy.Adam(learning_rate=LearningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    #Model is compiled\n",
    "    model.compile(optimizer=Opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n",
    "    Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "    Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "    XDataForecast=[]\n",
    "    # Flatten the double-nested lists\n",
    "    For_CV_GARCH = [item for sublist in For_CV_GARCH for item in sublist]\n",
    "    For_CV_GJR_GARCH = [item for sublist in For_CV_GJR_GARCH for item in sublist]\n",
    "    For_CV_TARCH = [item for sublist in For_CV_TARCH for item in sublist]\n",
    "    For_CV_EGARCH = [item for sublist in For_CV_EGARCH for item in sublist]\n",
    "    For_CV_AVGARCH = [item for sublist in For_CV_AVGARCH for item in sublist]\n",
    "    For_CV_FIGARCH = [item for sublist in For_CV_FIGARCH for item in sublist]\n",
    "    for i in range(len(For_CV_AVGARCH)):\n",
    "        forecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'], \n",
    "               'CV_GARCH' : For_CV_GARCH[i]/100, 'CV_GJR_GARCH' : For_CV_GJR_GARCH[i]/100, 'CV_TARCH' : For_CV_TARCH[i]/100, \n",
    "               'CV_EGARCH' : For_CV_EGARCH[i]/100, 'CV_AVGARCH' : For_CV_AVGARCH[i]/100, 'CV_FIGARCH' : For_CV_FIGARCH[i]/100}\n",
    "        XDataForecast.append(pd.DataFrame([forecast], index=[Index_Forecast]))\n",
    "    XDataForecast = pd.concat(XDataForecast)\n",
    "    return XDataForecast, Data_Forecast['DailyReturns']\n",
    "\n",
    "def T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n",
    "    XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n",
    "    XDataForecast = pd.concat([XData_AR,XDataForecast])\n",
    "    XDataForecastTotalScaled = Scaled_Norm.transform(XDataForecast)\n",
    "    XDataForecastTotalScaled_T, Y_T = Transformer_Database(Timestep, XDataForecastTotalScaled, np.zeros(XDataForecastTotalScaled.shape[0]))\n",
    "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T)\n",
    "    return TransformerPrediction[-1], XDataForecast.index[-1], TransformerPrediction[0:(XDataForecastTotalScaled_T.shape[0]-1)], ReturnForecast\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data for the Transformer model:\n",
    "\n",
    "In the Transformer_Database function, you need to adjust the data preparation process to handle the 4-step-ahead forecast vectors from the ARCH models. This likely involves changes to how the X and Y arrays are constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.05; LearningRate=0.01; Epochs = 100;BatchSize=64\n",
    "GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "#Database contaning AR models is generated\n",
    "Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                    CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "# #Original explanatory and response variables are generated\n",
    "XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "# #Data is normalized\n",
    "Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "#Data for fitting the transformer model is generated\n",
    "XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "\n",
    "XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_ANN_ARCH_Fit (Data,Database,Lag=1, LagSD=5, Timestep=10, Dropout=0.05, LearningRate=0.01, Epochs=1000, BatchSize=64):\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    #Database contaning AR models is generated\n",
    "    Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                        CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "    if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "    # #Original explanatory and response variables are generated\n",
    "    XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "    # #Data is normalized\n",
    "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "    #Data for fitting the transformer model is generated\n",
    "    XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "    #Model with transformer layer is defined\n",
    "    model = Transformer_Model(XData_AR_Norm_T.shape[1], XData_AR_Norm_T.shape[2], HeadsAttention=4, Dropout=Dropout, LearningRate=LearningRate)\n",
    "    model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, verbose=0, batch_size=BatchSize); tf.keras.backend.clear_session()\n",
    "    Forecast, Date_Forecast, TrainPrediction, ReturnForecast = T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n",
    "    return {'Date_Forecast':Date_Forecast,'Forecast_T_ANN_ARCH':Forecast}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XData_AR_Norm_T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XData_AR_Norm_T' is not defined"
     ]
    }
   ],
   "source": [
    "XData_AR_Norm_T.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2007, 12, 21), 'h1': 0.03164769, 'h2': 0.030543156, 'h3': 0.043870173, 'h4': 0.03504091}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2007, 12, 28), 'h1': 0.03508274, 'h2': 0.028066698, 'h3': 0.03169492, 'h4': 0.022067446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 1, 4), 'h1': 0.024130132, 'h2': 0.052720807, 'h3': 0.0453659, 'h4': 0.050918337}\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 1, 11), 'h1': 0.02959434, 'h2': 0.02978993, 'h3': 0.034462318, 'h4': 0.034021705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 1, 18), 'h1': 0.020165049, 'h2': 0.028361987, 'h3': 0.035944108, 'h4': 0.02331924}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 1, 25), 'h1': 0.026395217, 'h2': 0.027345365, 'h3': 0.031498536, 'h4': 0.030028865}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 2, 1), 'h1': 0.026110347, 'h2': 0.023337599, 'h3': 0.03166788, 'h4': 0.023489418}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 2, 8), 'h1': 0.027865779, 'h2': 0.030787215, 'h3': 0.039083455, 'h4': 0.02694654}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 2, 15), 'h1': 0.035448797, 'h2': 0.03200863, 'h3': 0.029135404, 'h4': 0.025185771}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 2, 22), 'h1': 0.03179861, 'h2': 0.029518992, 'h3': 0.027687075, 'h4': 0.021902189}\n",
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 2, 29), 'h1': 0.03383511, 'h2': 0.029017627, 'h3': 0.021686096, 'h4': 0.022804549}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 3, 7), 'h1': 0.03218161, 'h2': 0.029844627, 'h3': 0.028457964, 'h4': 0.023830602}\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 3, 14), 'h1': 0.0270559, 'h2': 0.028998323, 'h3': 0.043922342, 'h4': 0.028189788}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 3, 21), 'h1': 0.03725518, 'h2': 0.02818354, 'h3': 0.027215954, 'h4': 0.018270163}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 3, 28), 'h1': 0.028695831, 'h2': 0.021442728, 'h3': 0.028257182, 'h4': 0.03135871}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/volatility.py:2774: RuntimeWarning: overflow encountered in exp\n",
      "  paths[loc, :, :] = np.exp(_lnsigma2[:, m:])\n",
      "  4%|â–Ž         | 15/418 [01:19<35:23,  5.27s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[247], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m Data \u001b[39m=\u001b[39m DatabaseGeneration(Database, Lag, LagSD)\n\u001b[1;32m     18\u001b[0m \u001b[39m#Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m T_ANN_ARCH_Model \u001b[39m=\u001b[39m T_ANN_ARCH_Fit (Data,Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs)\n\u001b[1;32m     22\u001b[0m IterResults\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mDate_Forecast\u001b[39m\u001b[39m'\u001b[39m: T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mDate_Forecast\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdate(), \u001b[39m'\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m'\u001b[39m: T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mh2\u001b[39m\u001b[39m'\u001b[39m: T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m],\n\u001b[1;32m     23\u001b[0m              \u001b[39m'\u001b[39m\u001b[39mh3\u001b[39m\u001b[39m'\u001b[39m: T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mh4\u001b[39m\u001b[39m'\u001b[39m: T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m]}\n\u001b[1;32m     25\u001b[0m IterResults_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(IterResults,index \u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[158], line 21\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Fit\u001b[0;34m(Data, Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, BatchSize)\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[39m=\u001b[39m Transformer_Model(XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], HeadsAttention\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, Dropout\u001b[39m=\u001b[39mDropout, LearningRate\u001b[39m=\u001b[39mLearningRate)\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39mfit(XData_AR_Norm_T, YData_AR_Norm_T, epochs\u001b[39m=\u001b[39mEpochs, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, batch_size\u001b[39m=\u001b[39mBatchSize); tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mclear_session()\n\u001b[0;32m---> 21\u001b[0m Forecast, Date_Forecast, TrainPrediction, ReturnForecast \u001b[39m=\u001b[39m T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mDate_Forecast\u001b[39m\u001b[39m'\u001b[39m:Date_Forecast,\u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m:Forecast}\n",
      "Cell \u001b[0;32mIn[105], line 2\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Forecast\u001b[0;34m(Database, Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH, Scaled_Norm, XData_AR, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mT_ANN_ARCH_Forecast\u001b[39m (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n\u001b[0;32m----> 2\u001b[0m     XDataForecast, ReturnForecast \u001b[39m=\u001b[39m DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n\u001b[1;32m      3\u001b[0m     XDataForecast \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([XData_AR,XDataForecast])\n\u001b[1;32m      4\u001b[0m     XDataForecastTotalScaled \u001b[39m=\u001b[39m Scaled_Norm\u001b[39m.\u001b[39mtransform(XDataForecast)\n",
      "Cell \u001b[0;32mIn[136], line 220\u001b[0m, in \u001b[0;36mDatabaseGenerationForecast_AR\u001b[0;34m(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\u001b[0m\n\u001b[1;32m    216\u001b[0m For_CV_FIGARCH \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m For_CV_FIGARCH \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]\n\u001b[1;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(For_CV_AVGARCH)):\n\u001b[1;32m    218\u001b[0m     forecast\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mSD\u001b[39m\u001b[39m'\u001b[39m: Data_Forecast[\u001b[39m'\u001b[39m\u001b[39mSD\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m: Data_Forecast[\u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    219\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mCV_GARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_GARCH[i]\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_GJR_GARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_GJR_GARCH[i]\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_TARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_TARCH[i]\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \n\u001b[0;32m--> 220\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mCV_EGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_EGARCH[i]\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_AVGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_AVGARCH[i]\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_FIGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_FIGARCH[i]\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m}\n\u001b[1;32m    221\u001b[0m     XDataForecast\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39mDataFrame([forecast], index\u001b[39m=\u001b[39m[Index_Forecast]))\n\u001b[1;32m    222\u001b[0m XDataForecast \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(XDataForecast)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "asset = \"^GSPC\"\n",
    "# asset_name = re.sub('[\\W\\d_]+', '', asset)\n",
    "IndexEndDays=yf.download(asset,start=Start,  end=End, progress=False).resample('W-FRI').last().index\n",
    "\n",
    "Lag=1; LagSD=4; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100\n",
    "\n",
    "DataValidation = DatabaseGeneration(yf.download(asset,start='2000-01-01', end=date.today()+timedelta(days=1), progress=False).resample('W-FRI').last(), Lag, LagSD)\n",
    "\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'h1': [], 'h2': [], 'h3':[], 'h4': []})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(asset,start=IndexEndDays[i].date()-timedelta(days=780), end=IndexEndDays[i].date(), progress=False).resample('W-FRI').last()\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data,Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs)\n",
    "\n",
    "    \n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'h1': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][0], 'h2': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][1],\n",
    "                 'h3': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][2], 'h4': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][3]}\n",
    "    \n",
    "    IterResults_df = pd.DataFrame(IterResults,index =[0])\n",
    "    ResultsCollection = ResultsCollection.append(IterResults_df, ignore_index=True)\n",
    "\n",
    "\n",
    "    # ResultsCollection.to_csv(f'./assets/5_MTL_GARCH_{asset_name}.csv',index=False)\n",
    "    ResultsCollection.to_csv(f'./test.csv',index=False)\n",
    "    print(IterResults)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
