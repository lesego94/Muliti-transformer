{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006072,
     "end_time": "2020-10-03T04:16:28.188215",
     "exception": false,
     "start_time": "2020-10-03T04:16:28.182143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Install and call packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-03T04:16:28.217161Z",
     "iopub.status.busy": "2020-10-03T04:16:28.207154Z",
     "iopub.status.idle": "2020-10-03T04:16:48.157488Z",
     "shell.execute_reply": "2020-10-03T04:16:48.156959Z"
    },
    "papermill": {
     "duration": 19.963863,
     "end_time": "2020-10-03T04:16:48.157577",
     "exception": false,
     "start_time": "2020-10-03T04:16:28.193714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "import statsmodels.tsa.stattools as sts\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from scipy.stats.distributions import chi2 \n",
    "from arch import arch_model\n",
    "from math import sqrt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import scipy.optimize as sco\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.__future__ import reindexing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013216,
     "end_time": "2020-10-03T04:16:48.185228",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.172012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Define GARCH function (We simply import the data we already produced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T04:16:48.221045Z",
     "iopub.status.busy": "2020-10-03T04:16:48.220563Z",
     "iopub.status.idle": "2020-10-03T04:16:48.223396Z",
     "shell.execute_reply": "2020-10-03T04:16:48.222986Z"
    },
    "papermill": {
     "duration": 0.024144,
     "end_time": "2020-10-03T04:16:48.223469",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.199325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GARCH_predict(symbol_list, start, end, interval): \n",
    "    \n",
    "    #download data and calculate returns\n",
    "    data = yf.download(symbol_list, start, end, interval = interval)\n",
    "    ret = data.pct_change()['Adj Close']\n",
    "    ret = ret.dropna()\n",
    "    \n",
    "    #create list to store predicted variance and volatility\n",
    "    variance_list = []\n",
    "    vol_list = []\n",
    "    \n",
    "    for symbol in symbol_list:\n",
    "        \n",
    "        model = arch_model(ret[symbol], \n",
    "                            mean = \"Constant\",\n",
    "                            vol = \"GARCH\", \n",
    "                            dist = 'normal', \n",
    "                            p = 1, q = 1, \n",
    "                            rescale = False) \n",
    "       \n",
    "        result = model.fit(update_freq = 5, disp = 'off')\n",
    "        forecast = result.forecast()\n",
    "        \n",
    "        predict_var = (forecast.variance.iloc[-1]).iloc[0]\n",
    "        variance_list.append(predict_var)\n",
    "        vol_list.append(np.sqrt(predict_var))\n",
    "        \n",
    "        # It's optional to print other statistical result\n",
    "        # print(result.plot())\n",
    "        # print(result.summary())\n",
    "        # print(forecast.mean)\n",
    "\n",
    "    df = pd.DataFrame(columns = symbol_list, index = ['predicted var','predicted vol'])\n",
    "    df.loc['predicted var'] = variance_list\n",
    "    df.loc['predicted vol'] = vol_list\n",
    "    \n",
    "    # The function returns a DataFrame containing predicted variance and volatility values.\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013992,
     "end_time": "2020-10-03T04:16:48.251072",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.237080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Define Basic MVO Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T04:16:48.290143Z",
     "iopub.status.busy": "2020-10-03T04:16:48.286104Z",
     "iopub.status.idle": "2020-10-03T04:16:48.292961Z",
     "shell.execute_reply": "2020-10-03T04:16:48.292602Z"
    },
    "papermill": {
     "duration": 0.027765,
     "end_time": "2020-10-03T04:16:48.293033",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.265268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate):\n",
    "    \n",
    "    # Recall portfolio_annualised_performance(weights, mean_returns, cov_matrix) returns portfolio standard deviation and portfolio return\n",
    "    p_var = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    p_ret = np.sum(mean_returns*weights)\n",
    "    return -(p_ret - risk_free_rate/52) / p_var\n",
    "\n",
    "def max_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix, risk_free_rate)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bound = (0,0.25)\n",
    "    bounds = tuple(bound for asset in range(num_assets))\n",
    "    \n",
    "    result = sco.minimize(neg_sharpe_ratio, num_assets*[1./num_assets,], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result\n",
    "\n",
    "def MVO_result(df,mean_returns, cov_matrix, risk_free_rate):    \n",
    "\n",
    "    max_sharpe = max_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate)\n",
    "    print (\"-\"*80)\n",
    "    print (\"Maximum Sharpe Ratio Portfolio Allocation\\n\")\n",
    "    print (max_sharpe)\n",
    "    \n",
    "    weights = max_sharpe['x']\n",
    "    rp = np.sum(mean_returns*weights)\n",
    "    sdp = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    \n",
    "    \n",
    "    max_sharpe_allocation = pd.DataFrame(max_sharpe.x,index=df.columns,columns=['allocation'])\n",
    "    max_sharpe_allocation.allocation = [round(i*100,2)for i in max_sharpe_allocation.allocation]\n",
    "    max_sharpe_allocation = max_sharpe_allocation.T\n",
    "    \n",
    "    print (\"-\"*80)\n",
    "    print (\"Weekly Return:\", round(rp,5))\n",
    "    print (\"Weekly Volatility:\", round(sdp,5))\n",
    "    print (\"Max Weekly Sharpe Ratio:\", (rp - (risk_free_rate/52))/sdp)\n",
    "    print (\"\\n\")\n",
    "    print (max_sharpe_allocation)\n",
    "    return max_sharpe.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013281,
     "end_time": "2020-10-03T04:16:48.319985",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.306704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Seclet Stocks Based on Valuation Matrix Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013357,
     "end_time": "2020-10-03T04:16:48.346907",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.333550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The FactSet-sourced file contains valuation scores for SP500 composition stocks, categorized by industry. I re-grouped the data by narrowing industry types down to 9 types only: Financials, Chemicals, Tech, Utilities, Air, F&B, Oil, Services and Others. The criteria used in my trading is to select the stocks with best combined score in each industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T04:16:48.383144Z",
     "iopub.status.busy": "2020-10-03T04:16:48.382714Z",
     "iopub.status.idle": "2020-10-03T04:16:48.430661Z",
     "shell.execute_reply": "2020-10-03T04:16:48.430234Z"
    },
    "papermill": {
     "duration": 0.069821,
     "end_time": "2020-10-03T04:16:48.430759",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.360938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload stock score data scv file (downloaded from FactSet)\n",
    "stock = pd.read_csv('./files/Scoring the SP 500 - Valuation and Sales Growth.csv', na_values=['#N/A'])\n",
    "\n",
    "# set index by symbol\n",
    "stock = stock.set_index('Symbol')\n",
    "\n",
    "# look for the max score within each industry\n",
    "stock['score_max'] = stock.groupby(['Industry'])['Combined Score'].transform(max)\n",
    "\n",
    "# select stocks with industry max score \n",
    "selection = stock[stock['Combined Score']>=stock['score_max']*0.99]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01359,
     "end_time": "2020-10-03T04:16:48.458704",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.445114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Call Functions to Calculate Allocation for chozen stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  17 of 17 completed\n",
      "MVO result by historical covariance matrix\n",
      "[2.50000000e-01 6.60980435e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 4.25265173e-15 0.00000000e+00 0.00000000e+00\n",
      " 2.16254314e-01 2.71018421e-15 2.50000000e-01 0.00000000e+00\n",
      " 4.27828056e-15 2.17074581e-01 5.73061539e-04 0.00000000e+00\n",
      " 4.50628123e-15]\n"
     ]
    }
   ],
   "source": [
    "# input parameters\n",
    "# symbol_list = selection.index.tolist()\n",
    "symbol_list = ['AAPL', 'MSFT', 'NVDA', 'JNJ', 'NVS','JPM','GS','AMZN','DIS','MCD','NEE','BA','CAT','XOM','CVX','RIO','BHP']\n",
    "end = dt.datetime.now()\n",
    "start = end - dt.timedelta(140)\n",
    "interval = \"1wk\"\n",
    "\n",
    "# download data\n",
    "returns = yf.download(symbol_list, start, end, interval = interval).pct_change()['Adj Close'].dropna()\n",
    "mean_returns = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    "risk_free_rate = 0.12 / 100\n",
    "\n",
    "# print optimal allocation using historical covarianc matrix\n",
    "allocation_hist = MVO_result(returns, mean_returns, cov_matrix, risk_free_rate)\n",
    "print('MVO result by historical covariance matrix')\n",
    "print(allocation_hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the predicted variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal elements represent the variance of the returns of each asset. Variance is the square of volatility, so if you have the forecasted volatilities from your GARCH model, you would square these values to get the variance, and then place those on the diagonal of your covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define your file path\n",
    "File_path = \"../Output/assets/Drop000/\"\n",
    "Files = os.listdir(File_path)\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_var(date):\n",
    "    symbol_list = []\n",
    "    variance_list = []\n",
    "    # Initialize an empty DataFrame to store forecasts\n",
    "    Forecasts = pd.DataFrame(columns=['Asset', 'predicted var'])\n",
    "\n",
    "    # Loop through all files\n",
    "\n",
    "    for file in Files:\n",
    "        # Skip the \".DS_Store\" file\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "        \n",
    "        path = os.path.join(File_path, file)\n",
    "        asset_name = file.split('.')[0].split('_')[3]  # Assumes file name is the asset name\n",
    "        symbol_list.append(asset_name)\n",
    "        asset = pd.read_csv(path)\n",
    "        \n",
    "        # Get 'VaR_T_ANN_ARCH' for the given date\n",
    "        var_value = asset[asset['Date_Forecast'] == date]['Forecast_T_ANN_ARCH'].values[0]\n",
    "        variance_list.append(var_value)\n",
    "        \n",
    "    variance_list = np.reshape(variance_list, (1, -1))\n",
    "    # Create a DataFrame with the asset names as columns and predicted variances as the row\n",
    "    df = pd.DataFrame(variance_list, columns=symbol_list, index=['predicted var'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a054fff3446486ab6a5e8d7b7430652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='Start date', step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a07c155b034daca774d46104274bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatetimePicker(value=None, description='Start time')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab227bc353e4c99a71371cc72914f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='End date', step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "widget_start_date = widgets.DatePicker(\n",
    "    description='Start date',\n",
    "    disabled=False\n",
    ")\n",
    "widget_start_time = widgets.DatetimePicker(\n",
    "    description='Start time',\n",
    "    disabled=False\n",
    ")\n",
    "widget_end_date = widgets.DatePicker(\n",
    "    description='End date',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(widget_start_date, widget_start_time, widget_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MTL_GARCH_pred \u001b[39m=\u001b[39m retrieve_var(\u001b[39mstr\u001b[39;49m(widget_start_date\u001b[39m.\u001b[39;49mvalue))\n\u001b[1;32m      2\u001b[0m MTL_GARCH_pred\n",
      "Cell \u001b[0;32mIn[56], line 29\u001b[0m, in \u001b[0;36mretrieve_var\u001b[0;34m(date)\u001b[0m\n\u001b[1;32m     26\u001b[0m     asset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Get 'VaR_T_ANN_ARCH' for the given date\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     var_value \u001b[39m=\u001b[39m asset[asset[\u001b[39m'\u001b[39;49m\u001b[39mDate_Forecast\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m date][\u001b[39m'\u001b[39;49m\u001b[39mForecast_T_ANN_ARCH\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     30\u001b[0m     variance_list\u001b[39m.\u001b[39mappend(var_value)\n\u001b[1;32m     32\u001b[0m variance_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(variance_list, (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "MTL_GARCH_pred = retrieve_var(str(widget_start_date.value))\n",
    "MTL_GARCH_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T04:16:48.495885Z",
     "iopub.status.busy": "2020-10-03T04:16:48.493800Z",
     "iopub.status.idle": "2020-10-03T04:16:51.814835Z",
     "shell.execute_reply": "2020-10-03T04:16:51.814188Z"
    },
    "papermill": {
     "duration": 3.342511,
     "end_time": "2020-10-03T04:16:51.814952",
     "exception": false,
     "start_time": "2020-10-03T04:16:48.472441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Maximum Sharpe Ratio Portfolio Allocation\n",
      "\n",
      " message: Optimization terminated successfully\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -0.18537378065230703\n",
      "       x: [ 1.295e-01  1.318e-01 ...  1.935e-17  1.714e-17]\n",
      "     nit: 6\n",
      "     jac: [ 1.711e-02  1.715e-02 ...  5.225e-02  5.981e-02]\n",
      "    nfev: 108\n",
      "    njev: 6\n",
      "--------------------------------------------------------------------------------\n",
      "Weekly Return: 0.01647\n",
      "Weekly Volatility: 0.08875\n",
      "Max Weekly Sharpe Ratio: 0.18537378065230703\n",
      "\n",
      "\n",
      "             AAPL   AMZN    BA  BHP   CAT  CVX  DIS   GS   JNJ   JPM  MCD  \\\n",
      "allocation  12.95  13.18  0.29  0.0  1.02  0.0  0.0  0.0  4.44  1.92  8.1   \n",
      "\n",
      "             MSFT  NEE  NVDA    NVS  RIO  XOM  \n",
      "allocation  18.36  1.7  25.0  13.03  0.0  0.0  \n",
      "MVO result by GARCH-based covariance matrix\n",
      "[13. 13.  0.  0.  1.  0.  0.  0.  4.  2.  8. 18.  2. 25. 13.  0.  0.]\n",
      "--------------------------------------------------------------------------------\n",
      "MVO shrinkage result\n",
      "[1.89773165e-01 9.89313199e-02 1.43609598e-03 0.00000000e+00\n",
      " 5.09712305e-03 2.12632587e-15 1.78419020e-17 0.00000000e+00\n",
      " 1.30340679e-01 9.59284494e-03 1.65523332e-01 9.18134167e-02\n",
      " 8.51494994e-03 2.33537291e-01 6.54397836e-02 9.67392483e-18\n",
      " 2.26171005e-15]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# replace diagonal elements of cov matrix by GARCH-predicted variance.\n",
    "# GARCH_var = GARCH_predict(symbol_list, start, end, interval)\n",
    "\n",
    "adjust_cov_matrix =  cov_matrix.copy()\n",
    "for symbols in symbol_list:\n",
    "    adjust_cov_matrix[symbols][symbols] = MTL_GARCH_pred[symbols][0]\n",
    "\n",
    "# print optimal allocation using GARCH covariance matrix\n",
    "allocation_GARCH = MVO_result(returns, mean_returns, adjust_cov_matrix, risk_free_rate)\n",
    "print('MVO result by GARCH-based covariance matrix')\n",
    "print(np.rint(allocation_GARCH*100))\n",
    "\n",
    "# print the shrinkage allocation\n",
    "print('-'*80)\n",
    "print('MVO shrinkage result')\n",
    "print(0.5*allocation_hist + 0.5*allocation_GARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.29546330e-01, 1.31764596e-01, 2.87219196e-03, 0.00000000e+00,\n",
       "       1.01942461e-02, 0.00000000e+00, 3.56838040e-17, 0.00000000e+00,\n",
       "       4.44270438e-02, 1.91856899e-02, 8.10466633e-02, 1.83626833e-01,\n",
       "       1.70298999e-02, 2.50000000e-01, 1.30306506e-01, 1.93478497e-17,\n",
       "       1.71388647e-17])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocation_GARCH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problems\n",
    "I dont understand the interval. where do i specify a month re-balancing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time step get monthly "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace diagonal elements of cov matrix by MTL-GARCH-predicted variance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our case, we train a Transformer model for each asset to forecast the volatility for the next 4 time steps (weeks) and the last one will\n",
    "be the estimation of the future 1-month volatility. We then create a VCV matrix based on\n",
    "this prediction and the historical correlation matrix. which will be the input to the portfolio\n",
    "allocation problem described above.\n",
    "\n",
    "3. Iterate through MTL monthly. The time step we are on is (i - 1month) Basically we lag behind the prediction by a month. Bu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020498,
     "end_time": "2020-10-03T04:16:51.856935",
     "exception": false,
     "start_time": "2020-10-03T04:16:51.836437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Reference\n",
    "* https://campus.datacamp.com/courses/garch-models-in-python/garch-model-fundamentals?ex=9\n",
    "* https://stackoverflow.com/questions/59884917/forecasting-volatility-using-garch-in-python-arch-package\n",
    "* https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-count-in-groups-using-groupby\n",
    "* Professor Lee's BootCamp Videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back testing\n",
    "## generate portfolio returns starting from a period x. \n",
    "## switch out graphs for different dropouts\n",
    "## show the the allocation at each weekly time period."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "duration": 27.568463,
   "end_time": "2020-10-03T04:16:51.984868",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-03T04:16:24.416405",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
