{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pypfopt import risk_models\n",
    "from sklearn import preprocessing\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from datetime import date, datetime, timedelta\n",
    "from arch import arch_model\n",
    "from pypfopt import expected_returns\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DailyReturns</th>\n",
       "      <th>SD</th>\n",
       "      <th>TrueSD</th>\n",
       "      <th>DailyReturnsOld</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-02-06</th>\n",
       "      <td>0.050433</td>\n",
       "      <td>0.019049</td>\n",
       "      <td>0.054168</td>\n",
       "      <td>-0.007323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-13</th>\n",
       "      <td>-0.049271</td>\n",
       "      <td>0.041024</td>\n",
       "      <td>0.014027</td>\n",
       "      <td>0.050433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-20</th>\n",
       "      <td>-0.071156</td>\n",
       "      <td>0.042027</td>\n",
       "      <td>0.083498</td>\n",
       "      <td>-0.049271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-27</th>\n",
       "      <td>-0.046462</td>\n",
       "      <td>0.053521</td>\n",
       "      <td>0.077614</td>\n",
       "      <td>-0.071156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03-06</th>\n",
       "      <td>-0.072942</td>\n",
       "      <td>0.054168</td>\n",
       "      <td>0.074775</td>\n",
       "      <td>-0.046462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-11-16</th>\n",
       "      <td>-0.014578</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>-0.024589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-11-23</th>\n",
       "      <td>0.035590</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>0.017591</td>\n",
       "      <td>-0.014578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-11-30</th>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.035590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-07</th>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.026508</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>0.004976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-14</th>\n",
       "      <td>-0.003171</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.016361</td>\n",
       "      <td>0.001334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DailyReturns        SD    TrueSD  DailyReturnsOld\n",
       "Date                                                         \n",
       "2009-02-06      0.050433  0.019049  0.054168        -0.007323\n",
       "2009-02-13     -0.049271  0.041024  0.014027         0.050433\n",
       "2009-02-20     -0.071156  0.042027  0.083498        -0.049271\n",
       "2009-02-27     -0.046462  0.053521  0.077614        -0.071156\n",
       "2009-03-06     -0.072942  0.054168  0.074775        -0.046462\n",
       "...                  ...       ...       ...              ...\n",
       "2012-11-16     -0.014578  0.013409  0.020969        -0.024589\n",
       "2012-11-23      0.035590  0.010857  0.017591        -0.014578\n",
       "2012-11-30      0.004976  0.026365  0.006264         0.035590\n",
       "2012-12-07      0.001334  0.026508  0.012999         0.004976\n",
       "2012-12-14     -0.003171  0.020969  0.016361         0.001334\n",
       "\n",
       "[202 rows x 4 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = '2009-01-01';end = '2013-01-01'\n",
    "asset = \"^GSPC\"\n",
    "Lag=1; LagSD=4\n",
    "IndexEndDays=yf.download(asset,start=start,  end=end, progress=False).resample('W-FRI').last().index\n",
    "Database=yf.download(asset,start, end, progress=False).resample('W-FRI').last()\n",
    "Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.__future__ import reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting of GARCH(1,1)\n",
    "def GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GARCH11 = arch_model(AR_Data, dist ='t')\n",
    "    res_GARCH11 = GARCH11.fit(disp='off')\n",
    "    CV_GARCH11 = res_GARCH11.conditional_volatility\n",
    "    For_CV_GARCH11 = np.array(res_GARCH11.forecast(horizon=4).variance.dropna())\n",
    "    return GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11\n",
    "\n",
    "def TARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "    res_TARCH11 = TARCH11.fit(disp='off')\n",
    "    CV_TARCH11 = res_TARCH11.conditional_volatility\n",
    "    For_CV_TARCH11 = []\n",
    "    for i in range(4):\n",
    "        forecast = res_TARCH11.forecast(start=AR_Data.shape[0]-1, horizon=1)\n",
    "        For_CV_TARCH11.append(forecast.variance.iloc[-1,:].values[0])\n",
    "        AR_Data = np.append(AR_Data, forecast.mean.iloc[-1,:].values[0])\n",
    "        TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "        res_TARCH11 = TARCH11.fit(disp='off')\n",
    "    return TARCH11, res_TARCH11, CV_TARCH11, np.array(For_CV_TARCH11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11 = TARCH_Model_Student (Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.77134021, 3.72824687, 3.75234669, 3.71342018]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "For_CV_GARCH11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2009-01-05    2.950158\n",
       "2009-01-06    3.003343\n",
       "2009-01-07    2.914804\n",
       "2009-01-08    2.856434\n",
       "2009-01-09    2.769497\n",
       "                ...   \n",
       "2012-12-24    2.133043\n",
       "2012-12-26    2.055240\n",
       "2012-12-27    2.022124\n",
       "2012-12-28    1.951556\n",
       "2012-12-31    1.911728\n",
       "Name: cond_vol, Length: 1005, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_GARCH11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1, modify GARCH models to be multistep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find that the TARCH model does not support a horizon greater than 1, one workaround could be to implement recursive forecasting manually. This would involve using the model to make a one-step ahead forecast, appending that forecast to your time series, and then making the next one-step ahead forecast, and so on until you have made 4 forecasts. However, this approach would also be based on the assumption that future residuals are zero, and it would be computationally more intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data.dropna()\n",
    "\n",
    "#Fitting of GARCH(1,1)\n",
    "def GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GARCH11 = arch_model(AR_Data, dist ='t')\n",
    "    res_GARCH11 = GARCH11.fit(disp='off')\n",
    "    CV_GARCH11 = res_GARCH11.conditional_volatility\n",
    "    For_CV_GARCH11 = np.array(res_GARCH11.forecast(horizon=4).variance.dropna())\n",
    "    return GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11\n",
    "\n",
    "#Fitting of GJR_GARCH(1,1)\n",
    "def GJR_GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GJR_GARCH11 = arch_model(AR_Data, p=1, o=1, q=1, dist ='t')\n",
    "    res_GJR_GARCH11 = GJR_GARCH11.fit(disp='off')\n",
    "    CV_GJR_GARCH11 = res_GJR_GARCH11.conditional_volatility\n",
    "    For_CV_GJR_GARCH11 = np.array(res_GJR_GARCH11.forecast(horizon=4).variance.dropna())\n",
    "    return GJR_GARCH11, res_GJR_GARCH11, CV_GJR_GARCH11, For_CV_GJR_GARCH11\n",
    "\n",
    "#Fitting of TARCH(1,1)\n",
    "def TARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "    res_TARCH11 = TARCH11.fit(disp='off')\n",
    "    CV_TARCH11 = res_TARCH11.conditional_volatility\n",
    "    For_CV_TARCH11 = np.array(res_TARCH11.forecast(horizon=4,method= \"bootstrap\").variance.dropna())\n",
    "    return TARCH11, res_TARCH11, CV_TARCH11, For_CV_TARCH11\n",
    "\n",
    "#Fitting of EGARCH(1,1)\n",
    "def EGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    EGARCH11 = arch_model(AR_Data, dist ='t', vol=\"EGARCH\")\n",
    "    res_EGARCH11 = EGARCH11.fit(disp='off')\n",
    "    CV_EGARCH11 = res_EGARCH11.conditional_volatility\n",
    "    For_CV_EGARCH11 = np.array(res_EGARCH11.forecast(horizon=4,method=\"bootstrap\").variance.dropna())\n",
    "    return EGARCH11, res_EGARCH11,CV_EGARCH11, For_CV_EGARCH11\n",
    "\n",
    "#Fitting of Absolute Value GARCH(1,1)\n",
    "def AVGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    AVGARCH11 = arch_model(AR_Data, dist ='t', power=1)\n",
    "    res_AVGARCH11 = AVGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_AVGARCH11 = res_AVGARCH11.conditional_volatility\n",
    "    For_CV_AVGARCH11 = np.array(res_AVGARCH11.forecast(horizon=4,method=\"bootstrap\").variance.dropna())\n",
    "    return AVGARCH11, res_AVGARCH11, CV_AVGARCH11, For_CV_AVGARCH11\n",
    "\n",
    "#Fitting of FIGARCH11(1,1)\n",
    "def FIGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    FIGARCH11 = arch_model(AR_Data, dist ='t', vol=\"FIGARCH\")\n",
    "    res_FIGARCH11 = FIGARCH11.fit(disp='off')\n",
    "    CV_FIGARCH11 = res_FIGARCH11.conditional_volatility\n",
    "    For_CV_FIGARCH11 = np.array(res_FIGARCH11.forecast(horizon=4,method=\"bootstrap\").variance.dropna())\n",
    "    return FIGARCH11, res_FIGARCH11, CV_FIGARCH11, For_CV_FIGARCH11\n",
    "\n",
    "\n",
    "def Transformer_Database (Timestep, XData_AR, YData_AR):\n",
    "    Features = XData_AR.shape[1]\n",
    "    Sample = XData_AR.shape[0] - Timestep - 3  # Adjusted to allow for a 4-step-ahead target\n",
    "    XDataTrainScaledRNN = np.zeros([Sample, Timestep, Features])\n",
    "    YDataTrainRNN = np.zeros([Sample, 4])  # Adjusted for 4-step-ahead forecasts\n",
    "    \n",
    "    for i in range(Sample):\n",
    "        XDataTrainScaledRNN[i,:,:] = XData_AR[i:(Timestep+i)]\n",
    "        YDataTrainRNN[i, :] = YData_AR[(Timestep+i):(Timestep+i+4)]  # 4-step-ahead target\n",
    "    \n",
    "    return XDataTrainScaledRNN, YDataTrainRNN\n",
    "\n",
    "#MultiHeadSelfAttention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "        \n",
    "#Transformer Keras Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.nb_dict = {}; self.Bagging=5\n",
    "        for i in range(self.Bagging):\n",
    "          self.nb_dict[\"att{0}\".format(i)]=MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    def call(self, inputs, training):\n",
    "        self.att_dict = {}\n",
    "        for i in range(self.Bagging):\n",
    "          self.att_dict[\"att{0}\".format(i)]=self.nb_dict[\"att{0}\".format(i)](tf.keras.layers.Dropout(.1)(inputs))\n",
    "          if i==0: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"att{0}\".format(i)]/self.Bagging \n",
    "          else: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"attn_output\"]+self.att_dict[\"att{0}\".format(i)]/self.Bagging\n",
    "        attn_output = self.dropout1(self.att_dict[\"attn_output\"], training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "#Database is calculated\n",
    "def DatabaseGenerationForecast (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data\n",
    "\n",
    "def Transformer_Model (Shape1, Shape2, HeadsAttention,Dropout, LearningRate):\n",
    "    #Model struture is defined\n",
    "    Input = tf.keras.Input(shape=(Shape1,Shape2), name=\"Input\")\n",
    "    #LSTM is applied on top of the transformer\n",
    "    X = tf.keras.layers.LSTM(units=16, dropout=Dropout, return_sequences=True)(Input)\n",
    "    #Tranformer architecture is implemented\n",
    "    transformer_block_1 = TransformerBlock(embed_dim=16, num_heads=HeadsAttention, ff_dim=8, rate=Dropout)\n",
    "    X = transformer_block_1(X)\n",
    "    #Dense layers are used\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dense(8, activation=tf.nn.sigmoid)(X)\n",
    "    X = tf.keras.layers.Dropout(Dropout)(X)\n",
    "    Output = tf.keras.layers.Dense(4, activation=tf.nn.sigmoid, name=\"Output\")(X)\n",
    "    model = tf.keras.Model(inputs=Input, outputs=Output)\n",
    "    #Optimizer is defined\n",
    "    Opt = tf.keras.optimizers.legacy.Adam(learning_rate=LearningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    #Model is compiled\n",
    "    model.compile(optimizer=Opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "#Final AR database for forcasting is generated\n",
    "# def DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n",
    "#     Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "#     Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "#     XDataForecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'], \n",
    "#                'CV_GARCH' : For_CV_GARCH/100, 'CV_GJR_GARCH' : For_CV_GJR_GARCH/100, 'CV_TARCH' : For_CV_TARCH/100, \n",
    "#                'CV_EGARCH' : For_CV_EGARCH/100, 'CV_AVGARCH' : For_CV_AVGARCH/100, 'CV_FIGARCH' : For_CV_FIGARCH/100}\n",
    "    \n",
    "    \n",
    "#     return pd.DataFrame([XDataForecast], index=[Index_Forecast]), Data_Forecast['DailyReturns']\n",
    "\n",
    "def DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n",
    "    Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "    Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "    XDataForecast=[]\n",
    "    # Flatten the double-nested lists\n",
    "    For_CV_GARCH = [item for sublist in For_CV_GARCH for item in sublist]\n",
    "    For_CV_GJR_GARCH = [item for sublist in For_CV_GJR_GARCH for item in sublist]\n",
    "    For_CV_TARCH = [item for sublist in For_CV_TARCH for item in sublist]\n",
    "    For_CV_EGARCH = [item for sublist in For_CV_EGARCH for item in sublist]\n",
    "    For_CV_AVGARCH = [item for sublist in For_CV_AVGARCH for item in sublist]\n",
    "    For_CV_FIGARCH = [item for sublist in For_CV_FIGARCH for item in sublist]\n",
    "    for i in range(len(For_CV_AVGARCH)):\n",
    "        forecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'], \n",
    "               'CV_GARCH' : For_CV_GARCH[i]/100, 'CV_GJR_GARCH' : For_CV_GJR_GARCH[i]/100, 'CV_TARCH' : For_CV_TARCH[i]/100, \n",
    "               'CV_EGARCH' : For_CV_EGARCH[i]/100, 'CV_AVGARCH' : For_CV_AVGARCH[i]/100, 'CV_FIGARCH' : For_CV_FIGARCH[i]/100}\n",
    "        XDataForecast.append(pd.DataFrame([forecast], index=[Index_Forecast]))\n",
    "    XDataForecast = pd.concat(XDataForecast)\n",
    "    return XDataForecast, Data_Forecast['DailyReturns']\n",
    "\n",
    "def T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n",
    "    XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n",
    "    XDataForecast = pd.concat([XData_AR,XDataForecast])\n",
    "    XDataForecastTotalScaled = Scaled_Norm.transform(XDataForecast)\n",
    "    XDataForecastTotalScaled_T, Y_T = Transformer_Database(Timestep, XDataForecastTotalScaled, np.zeros(XDataForecastTotalScaled.shape[0]))\n",
    "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T)\n",
    "    return TransformerPrediction[-1], XDataForecast.index[-1], TransformerPrediction[0:(XDataForecastTotalScaled_T.shape[0]-1)], ReturnForecast\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data for the Transformer model:\n",
    "\n",
    "In the Transformer_Database function, you need to adjust the data preparation process to handle the 4-step-ahead forecast vectors from the ARCH models. This likely involves changes to how the X and Y arrays are constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.05; LearningRate=0.01; Epochs = 100;BatchSize=64\n",
    "GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "#Database contaning AR models is generated\n",
    "Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                    CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "# #Original explanatory and response variables are generated\n",
    "XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "# #Data is normalized\n",
    "Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "#Data for fitting the transformer model is generated\n",
    "XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "\n",
    "XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DailyReturns</th>\n",
       "      <th>SD</th>\n",
       "      <th>TrueSD</th>\n",
       "      <th>DailyReturnsOld</th>\n",
       "      <th>CV_GARCH</th>\n",
       "      <th>CV_GJR_GARCH</th>\n",
       "      <th>CV_TARCH</th>\n",
       "      <th>CV_EGARCH</th>\n",
       "      <th>CV_AVGARCH</th>\n",
       "      <th>CV_FIGARCH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-01-06</th>\n",
       "      <td>0.029334</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>-0.016187</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.011925</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.011497</td>\n",
       "      <td>0.015885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-13</th>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.029334</td>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.015885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-20</th>\n",
       "      <td>-0.020494</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.014798</td>\n",
       "      <td>0.014577</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>0.013978</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>0.017552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-27</th>\n",
       "      <td>0.017469</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>-0.020494</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>0.015022</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.012506</td>\n",
       "      <td>0.011753</td>\n",
       "      <td>0.016674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-03</th>\n",
       "      <td>-0.015457</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.017469</td>\n",
       "      <td>0.014882</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>0.013582</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.017334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-30</th>\n",
       "      <td>0.027683</td>\n",
       "      <td>0.016988</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.012444</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.020528</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>0.018683</td>\n",
       "      <td>0.019954</td>\n",
       "      <td>0.021482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-07</th>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.027499</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>0.018489</td>\n",
       "      <td>0.020407</td>\n",
       "      <td>0.020355</td>\n",
       "      <td>0.017175</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>0.020302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-14</th>\n",
       "      <td>-0.024700</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.024997</td>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.018523</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.017082</td>\n",
       "      <td>0.020125</td>\n",
       "      <td>0.020625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-21</th>\n",
       "      <td>0.011184</td>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.024700</td>\n",
       "      <td>0.018556</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.021044</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>0.020210</td>\n",
       "      <td>0.019837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-28</th>\n",
       "      <td>-0.004030</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>0.011184</td>\n",
       "      <td>0.018590</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.015718</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>0.020537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DailyReturns        SD    TrueSD  DailyReturnsOld  CV_GARCH  \\\n",
       "Date                                                                      \n",
       "2006-01-06      0.029334  0.009636  0.021546        -0.016187  0.014713   \n",
       "2006-01-13      0.001679  0.018781  0.017285         0.029334  0.014755   \n",
       "2006-01-20     -0.020494  0.018827  0.017363         0.001679  0.014798   \n",
       "2006-01-27      0.017469  0.022637  0.015263        -0.020494  0.014840   \n",
       "2006-02-03     -0.015457  0.021546  0.012829         0.017469  0.014882   \n",
       "...                  ...       ...       ...              ...       ...   \n",
       "2007-11-30      0.027683  0.016988  0.022553        -0.012444  0.018455   \n",
       "2007-12-07      0.015755  0.027499  0.018247         0.027683  0.018489   \n",
       "2007-12-14     -0.024700  0.017172  0.024997         0.015755  0.018523   \n",
       "2007-12-21      0.011184  0.024287  0.024463        -0.024700  0.018556   \n",
       "2007-12-28     -0.004030  0.022553  0.024410         0.011184  0.018590   \n",
       "\n",
       "            CV_GJR_GARCH  CV_TARCH  CV_EGARCH  CV_AVGARCH  CV_FIGARCH  \n",
       "Date                                                                   \n",
       "2006-01-06      0.014710  0.011925   0.014632    0.011497    0.015885  \n",
       "2006-01-13      0.014643  0.011806   0.012560    0.011582    0.015885  \n",
       "2006-01-20      0.014577  0.011745   0.013978    0.011668    0.017552  \n",
       "2006-01-27      0.015022  0.013926   0.012506    0.011753    0.016674  \n",
       "2006-02-03      0.014951  0.013582   0.011703    0.011839    0.017334  \n",
       "...                  ...       ...        ...         ...         ...  \n",
       "2007-11-30      0.020528  0.021559   0.018683    0.019954    0.021482  \n",
       "2007-12-07      0.020407  0.020355   0.017175    0.020039    0.020302  \n",
       "2007-12-14      0.020287  0.019287   0.017082    0.020125    0.020625  \n",
       "2007-12-21      0.020686  0.021044   0.015372    0.020210    0.019837  \n",
       "2007-12-28      0.020563  0.019898   0.015718    0.020296    0.020537  \n",
       "\n",
       "[104 rows x 10 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_ANN_ARCH_Fit (Data,Database,Lag=1, LagSD=5, Timestep=10, Dropout=0.05, LearningRate=0.01, Epochs=1000, BatchSize=64):\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    #Database contaning AR models is generated\n",
    "    Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                        CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "    if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "    # #Original explanatory and response variables are generated\n",
    "    XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "    # #Data is normalized\n",
    "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "    #Data for fitting the transformer model is generated\n",
    "    XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "    #Model with transformer layer is defined\n",
    "    model = Transformer_Model(XData_AR_Norm_T.shape[1], XData_AR_Norm_T.shape[2], HeadsAttention=4, Dropout=Dropout, LearningRate=LearningRate)\n",
    "    model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, verbose=0, batch_size=BatchSize); tf.keras.backend.clear_session()\n",
    "    Forecast, Date_Forecast, TrainPrediction, ReturnForecast = T_ANN_ARCH_Forecast (Database,Timestep, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n",
    "    return {'Date_Forecast':Date_Forecast,'Forecast_T_ANN_ARCH':Forecast}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DailyReturns</th>\n",
       "      <th>SD</th>\n",
       "      <th>TrueSD</th>\n",
       "      <th>DailyReturnsOld</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-01-06</th>\n",
       "      <td>0.029334</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>-0.016187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-13</th>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.029334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-20</th>\n",
       "      <td>-0.020494</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>0.001679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-27</th>\n",
       "      <td>0.017469</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>-0.020494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-03</th>\n",
       "      <td>-0.015457</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.017469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-30</th>\n",
       "      <td>0.027683</td>\n",
       "      <td>0.016988</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.012444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-07</th>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.027499</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.027683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-14</th>\n",
       "      <td>-0.024700</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.024997</td>\n",
       "      <td>0.015755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-21</th>\n",
       "      <td>0.011184</td>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>-0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-28</th>\n",
       "      <td>-0.004030</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>0.011184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DailyReturns        SD    TrueSD  DailyReturnsOld\n",
       "Date                                                         \n",
       "2006-01-06      0.029334  0.009636  0.021546        -0.016187\n",
       "2006-01-13      0.001679  0.018781  0.017285         0.029334\n",
       "2006-01-20     -0.020494  0.018827  0.017363         0.001679\n",
       "2006-01-27      0.017469  0.022637  0.015263        -0.020494\n",
       "2006-02-03     -0.015457  0.021546  0.012829         0.017469\n",
       "...                  ...       ...       ...              ...\n",
       "2007-11-30      0.027683  0.016988  0.022553        -0.012444\n",
       "2007-12-07      0.015755  0.027499  0.018247         0.027683\n",
       "2007-12-14     -0.024700  0.017172  0.024997         0.015755\n",
       "2007-12-21      0.011184  0.024287  0.024463        -0.024700\n",
       "2007-12-28     -0.004030  0.022553  0.024410         0.011184\n",
       "\n",
       "[104 rows x 4 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "T_ANN_ARCH_Model  = T_ANN_ARCH_Fit(Data,Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs)\n",
    "Predictions = pd.DataFrame({'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "asset = \"^GSPC\"\n",
    "# asset_name = re.sub('[\\W\\d_]+', '', asset)\n",
    "IndexEndDays=yf.download(asset,start=Start,  end=End, progress=False).resample('W-FRI').last().index\n",
    "\n",
    "Lag=1; LagSD=4; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100; Alpha=0.005; DF=4\n",
    "\n",
    "DataValidation = DatabaseGeneration(yf.download(asset,start='2000-01-01', end=date.today()+timedelta(days=1), progress=False).resample('W-FRI').last(), Lag, LagSD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04479299"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DailyReturns</th>\n",
       "      <th>SD</th>\n",
       "      <th>TrueSD</th>\n",
       "      <th>DailyReturnsOld</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-02-08</th>\n",
       "      <td>-0.047047</td>\n",
       "      <td>0.042420</td>\n",
       "      <td>0.026639</td>\n",
       "      <td>0.047558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-15</th>\n",
       "      <td>0.013949</td>\n",
       "      <td>0.048085</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>-0.047047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-22</th>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.039160</td>\n",
       "      <td>0.013653</td>\n",
       "      <td>0.013949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-29</th>\n",
       "      <td>-0.016753</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>0.025985</td>\n",
       "      <td>0.002308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-03-07</th>\n",
       "      <td>-0.028401</td>\n",
       "      <td>0.026639</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>-0.016753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-13</th>\n",
       "      <td>-0.036955</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.028272</td>\n",
       "      <td>0.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-20</th>\n",
       "      <td>0.032165</td>\n",
       "      <td>0.024985</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>-0.036955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-27</th>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.028771</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.032165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-04</th>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.028775</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-11</th>\n",
       "      <td>-0.038659</td>\n",
       "      <td>0.028272</td>\n",
       "      <td>0.026989</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>410 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DailyReturns        SD    TrueSD  DailyReturnsOld\n",
       "Date                                                         \n",
       "2008-02-08     -0.047047  0.042420  0.026639         0.047558\n",
       "2008-02-15      0.013949  0.048085  0.018960        -0.047047\n",
       "2008-02-22      0.002308  0.039160  0.013653         0.013949\n",
       "2008-02-29     -0.016753  0.039178  0.025985         0.002308\n",
       "2008-03-07     -0.028401  0.026639  0.025200        -0.016753\n",
       "...                  ...       ...       ...              ...\n",
       "2015-11-13     -0.036955  0.007619  0.028272         0.009496\n",
       "2015-11-20      0.032165  0.024985  0.029000        -0.036955\n",
       "2015-11-27      0.000450  0.028771  0.019059         0.032165\n",
       "2015-12-04      0.000756  0.028775  0.027086         0.000450\n",
       "2015-12-11     -0.038659  0.028272  0.026989         0.000756\n",
       "\n",
       "[410 rows x 4 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataValidation = DatabaseGeneration(yf.download(asset,start='2008-01-01', end=date(2015,12,31)+timedelta(days=1), progress=False).resample('W-FRI').last(), Lag, LagSD)\n",
    "DataValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2007, 12, 21), 'h1': 0.025709957, 'h2': 0.021212365, 'h3': 0.032684866, 'h4': 0.032770135}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2007, 12, 28), 'h1': 0.048752606, 'h2': 0.04943467, 'h3': 0.040322587, 'h4': 0.048031487}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/univariate/base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date_Forecast': datetime.date(2008, 1, 4), 'h1': 0.05105301, 'h2': 0.0433832, 'h3': 0.056517262, 'h4': 0.05736131}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/418 [00:19<45:30,  6.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[230], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m#Loop for generating the results\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(IndexEndDays\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])):\n\u001b[1;32m     14\u001b[0m     \u001b[39m#Database is downloaded from yahoo finance and lag of returns defined\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     Database\u001b[39m=\u001b[39myf\u001b[39m.\u001b[39;49mdownload(asset,start\u001b[39m=\u001b[39;49mIndexEndDays[i]\u001b[39m.\u001b[39;49mdate()\u001b[39m-\u001b[39;49mtimedelta(days\u001b[39m=\u001b[39;49m\u001b[39m780\u001b[39;49m), end\u001b[39m=\u001b[39;49mIndexEndDays[i]\u001b[39m.\u001b[39;49mdate(), progress\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mW-FRI\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlast()\n\u001b[1;32m     16\u001b[0m     \u001b[39m#Database for fitting the models is generated\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     Data \u001b[39m=\u001b[39m DatabaseGeneration(Database, Lag, LagSD)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/yfinance/multi.py:131\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, show_errors, interval, prepost, proxy, rounding, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         _download_one_threaded(ticker, period\u001b[39m=\u001b[39mperiod, interval\u001b[39m=\u001b[39minterval,\n\u001b[1;32m    125\u001b[0m                                start\u001b[39m=\u001b[39mstart, end\u001b[39m=\u001b[39mend, prepost\u001b[39m=\u001b[39mprepost,\n\u001b[1;32m    126\u001b[0m                                actions\u001b[39m=\u001b[39mactions, auto_adjust\u001b[39m=\u001b[39mauto_adjust,\n\u001b[1;32m    127\u001b[0m                                back_adjust\u001b[39m=\u001b[39mback_adjust, repair\u001b[39m=\u001b[39mrepair, keepna\u001b[39m=\u001b[39mkeepna,\n\u001b[1;32m    128\u001b[0m                                progress\u001b[39m=\u001b[39m(progress \u001b[39mand\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m), proxy\u001b[39m=\u001b[39mproxy,\n\u001b[1;32m    129\u001b[0m                                rounding\u001b[39m=\u001b[39mrounding, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    130\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(shared\u001b[39m.\u001b[39m_DFS) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(tickers):\n\u001b[0;32m--> 131\u001b[0m         _time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m    133\u001b[0m \u001b[39m# download synchronously\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[39mfor\u001b[39;00m i, ticker \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tickers):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "asset = \"^GSPC\"\n",
    "# asset_name = re.sub('[\\W\\d_]+', '', asset)\n",
    "IndexEndDays=yf.download(asset,start=Start,  end=End, progress=False).resample('W-FRI').last().index\n",
    "\n",
    "Lag=1; LagSD=4; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100\n",
    "\n",
    "DataValidation = DatabaseGeneration(yf.download(asset,start='2000-01-01', end=date.today()+timedelta(days=1), progress=False).resample('W-FRI').last(), Lag, LagSD)\n",
    "\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'h1': [], 'h2': [], 'h3':[], 'h4': []})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(asset,start=IndexEndDays[i].date()-timedelta(days=780), end=IndexEndDays[i].date(), progress=False).resample('W-FRI').last()\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data,Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs)\n",
    "\n",
    "    \n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'h1': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][0], 'h2': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][1],\n",
    "                 'h3': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][2], 'h4': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'][3]}\n",
    "    \n",
    "    IterResults_df = pd.DataFrame(IterResults,index =[0])\n",
    "    ResultsCollection = ResultsCollection.append(IterResults_df, ignore_index=True)\n",
    "\n",
    "\n",
    "    # ResultsCollection.to_csv(f'./assets/5_MTL_GARCH_{asset_name}.csv',index=False)\n",
    "    ResultsCollection.to_csv(f'./test.csv',index=False)\n",
    "    print(IterResults)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
