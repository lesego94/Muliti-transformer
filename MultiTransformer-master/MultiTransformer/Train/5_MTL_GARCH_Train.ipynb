{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source notebook is loaded\n",
    "\n",
    "All the libraries and formulas needed for running the MTL-GARCH model are loaded. Please install all the libraries before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# python -m pip install yahoo-finance; pip install yahoo-finance; pip install yfinance --upgrade --no-cache-dir\n",
    "import yfinance as yf\n",
    "# !pip install --upgrade ta; pip install ta\n",
    "import ta as ta\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import t\n",
    "import tensorflow as tf\n",
    "from datetime import date, datetime, timedelta\n",
    "from arch import arch_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data.dropna()\n",
    "\n",
    "#Fitting of GARCH(1,1)\n",
    "def GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GARCH11 = arch_model(AR_Data, dist ='t')\n",
    "    res_GARCH11 = GARCH11.fit(disp='off')\n",
    "    CV_GARCH11 = res_GARCH11.conditional_volatility\n",
    "    For_CV_GARCH11 = np.array(res_GARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11\n",
    "\n",
    "#Fitting of GJR_GARCH(1,1)\n",
    "def GJR_GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GJR_GARCH11 = arch_model(AR_Data, p=1, o=1, q=1, dist ='t')\n",
    "    res_GJR_GARCH11 = GJR_GARCH11.fit(disp='off')\n",
    "    CV_GJR_GARCH11 = res_GJR_GARCH11.conditional_volatility\n",
    "    For_CV_GJR_GARCH11 = np.array(res_GJR_GARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return GJR_GARCH11, res_GJR_GARCH11, CV_GJR_GARCH11, For_CV_GJR_GARCH11\n",
    "\n",
    "#Fitting of TARCH(1,1)\n",
    "def TARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "    res_TARCH11 = TARCH11.fit(disp='off')\n",
    "    CV_TARCH11 = res_TARCH11.conditional_volatility\n",
    "    For_CV_TARCH11 = np.array(res_TARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return TARCH11, res_TARCH11, CV_TARCH11, For_CV_TARCH11\n",
    "\n",
    "#Fitting of EGARCH(1,1)\n",
    "def EGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    EGARCH11 = arch_model(AR_Data, dist ='t', vol=\"EGARCH\")\n",
    "    res_EGARCH11 = EGARCH11.fit(disp='off')\n",
    "    CV_EGARCH11 = res_EGARCH11.conditional_volatility\n",
    "    For_CV_EGARCH11 = np.array(res_EGARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return EGARCH11, res_EGARCH11,CV_EGARCH11, For_CV_EGARCH11\n",
    "\n",
    "#Fitting of Absolute Value GARCH(1,1)\n",
    "def AVGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    AVGARCH11 = arch_model(AR_Data, dist ='t', power=1)\n",
    "    res_AVGARCH11 = AVGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_AVGARCH11 = res_AVGARCH11.conditional_volatility\n",
    "    For_CV_AVGARCH11 = np.array(res_AVGARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return AVGARCH11, res_AVGARCH11, CV_AVGARCH11, For_CV_AVGARCH11\n",
    "\n",
    "#Fitting of FIGARCH11(1,1)\n",
    "def FIGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    FIGARCH11 = arch_model(AR_Data, dist ='t', vol=\"FIGARCH\")\n",
    "    res_FIGARCH11 = FIGARCH11.fit(disp='off')\n",
    "    CV_FIGARCH11 = res_FIGARCH11.conditional_volatility\n",
    "    For_CV_FIGARCH11 = np.array(res_FIGARCH11.forecast(horizon=1).variance.dropna())[0][0]   \n",
    "    return FIGARCH11, res_FIGARCH11, CV_FIGARCH11, For_CV_FIGARCH11\n",
    "\n",
    "#AR models are fitted. As requested by arma package, returns are multiplied by 100 in order to improve the fitting process.\n",
    "#GARCH(1,1), GJR_GARCH(1,1), TARCH(1,1), EGARCH(1,1), AVGARCH(1,1) and FIGARCH(1,1) volatility models are fitted.\n",
    "#T student is assumed as distribution.\n",
    "def AR_Models (Data):\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    return GARCH_Parameters, CV_GARCH, For_CV_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH\n",
    "\n",
    "#MultiHeadSelfAttention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "        \n",
    "#Transformer Keras Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.nb_dict = {}; self.Bagging=5\n",
    "        for i in range(self.Bagging):\n",
    "          self.nb_dict[\"att{0}\".format(i)]=MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    def call(self, inputs, training):\n",
    "        self.att_dict = {}\n",
    "        for i in range(self.Bagging):\n",
    "          self.att_dict[\"att{0}\".format(i)]=self.nb_dict[\"att{0}\".format(i)](tf.keras.layers.Dropout(.1)(inputs))\n",
    "          if i==0: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"att{0}\".format(i)]/self.Bagging \n",
    "          else: \n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"attn_output\"]+self.att_dict[\"att{0}\".format(i)]/self.Bagging\n",
    "        attn_output = self.dropout1(self.att_dict[\"attn_output\"], training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def Transformer_Model (Shape1, Shape2, HeadsAttention,Dropout, LearningRate):\n",
    "    #Model struture is defined\n",
    "    Input = tf.keras.Input(shape=(Shape1,Shape2), name=\"Input\")\n",
    "    #LSTM is applied on top of the transformer\n",
    "    X = tf.keras.layers.LSTM(units=16, dropout=Dropout, return_sequences=True)(Input)\n",
    "    #Tranformer architecture is implemented\n",
    "    transformer_block_1 = TransformerBlock(embed_dim=16, num_heads=HeadsAttention, ff_dim=8, rate=Dropout)\n",
    "    X = transformer_block_1(X)\n",
    "    #Dense layers are used\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dense(8, activation=tf.nn.sigmoid)(X)\n",
    "    X = tf.keras.layers.Dropout(Dropout)(X)\n",
    "    Output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, name=\"Output\")(X)\n",
    "    model = tf.keras.Model(inputs=Input, outputs=Output)\n",
    "    #Optimizer is defined\n",
    "    Opt = tf.keras.optimizers.Adam(learning_rate=LearningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    #Model is compiled\n",
    "    model.compile(optimizer=Opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "#It generates the database for fitting transformer. No positional encoding is needed as LSTM plays this role in the model structure\n",
    "def Transformer_Database (Timestep, XData_AR, YData_AR):\n",
    "    Features = XData_AR.shape[1]; Sample = XData_AR.shape[0]-Timestep+1\n",
    "    XDataTrainScaledRNN=np.zeros([Sample, Timestep, Features]); YDataTrainRNN=np.zeros([Sample])\n",
    "    for i in range(Sample):\n",
    "        XDataTrainScaledRNN[i,:,:] = XData_AR[i:(Timestep+i)]\n",
    "        YDataTrainRNN[i] = YData_AR[Timestep+i-1]\n",
    "    return XDataTrainScaledRNN, YDataTrainRNN\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGenerationForecast (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data\n",
    "\n",
    "#Final AR database for forcasting is generated\n",
    "def DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n",
    "    Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "    Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "    XDataForecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'], \n",
    "               'CV_GARCH' : For_CV_GARCH/100, 'CV_GJR_GARCH' : For_CV_GJR_GARCH/100, 'CV_TARCH' : For_CV_TARCH/100, \n",
    "               'CV_EGARCH' : For_CV_EGARCH/100, 'CV_AVGARCH' : For_CV_AVGARCH/100, 'CV_FIGARCH' : For_CV_FIGARCH/100}\n",
    "    return pd.DataFrame([XDataForecast], index=[Index_Forecast]), Data_Forecast['DailyReturns']\n",
    "\n",
    "#Transformed ANN-ARCH model forecast\n",
    "def T_ANN_ARCH_Forecast (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n",
    "    XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n",
    "    XDataForecast = pd.concat([XData_AR,XDataForecast])\n",
    "    XDataForecastTotalScaled = Scaled_Norm.transform(XDataForecast)\n",
    "    XDataForecastTotalScaled_T, Y_T = Transformer_Database(Timestep, XDataForecastTotalScaled, np.zeros(XDataForecastTotalScaled.shape[0]))\n",
    "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T)\n",
    "    return TransformerPrediction[-1][0], XDataForecast.index[-1], TransformerPrediction[0:(XDataForecastTotalScaled_T.shape[0]-1)], ReturnForecast\n",
    "\n",
    "#It calculates VaR taking into consideration the forecasted sigma to calculate the scale parameter\n",
    "def T_ANN_ARCH_VaR (Alpha, HistoricalReturns, ForecastedSigma, DF):\n",
    "    HistoricalMean = np.mean(HistoricalReturns)\n",
    "    ScaleParameter = np.sqrt((ForecastedSigma**2)*((DF-2)/DF))\n",
    "    VaR = -t.ppf(Alpha, DF, loc=HistoricalMean, scale=ScaleParameter)\n",
    "    return VaR\n",
    "\n",
    "#Formula to calculate the VaR of ARCH models\n",
    "def VaR_AR_Model (AR_Model,AR_Model_Results,Alpha):\n",
    "    Cond_Var=AR_Model_Results.forecast(horizon=1).variance.dropna()\n",
    "    Cond_Mean=AR_Model_Results.forecast(horizon=1).mean.dropna()\n",
    "    Quantile_Dist=AR_Model.distribution.ppf([Alpha], AR_Model_Results.params[-1:])\n",
    "    VaR=(-Cond_Mean-np.sqrt(Cond_Var)*Quantile_Dist)/100\n",
    "    return VaR.values\n",
    "\n",
    "#Formula to calculate the VaR of all the ARCH models\n",
    "def VaR_AR_Total(Alpha, GARCH_fit, GJR_GARCH_fit, TARCH_fit, EGARCH_fit, AVGARCH_fit, FIGARCH_fit,GARCH, GJR_GARCH, TARCH, EGARCH, AVGARCH, FIGARCH):\n",
    "    VaR_GARCH = VaR_AR_Model (GARCH,GARCH_fit,Alpha)\n",
    "    VaR_GJR_GARCH = VaR_AR_Model (GJR_GARCH,GJR_GARCH_fit,Alpha)\n",
    "    VaR_TARCH = VaR_AR_Model (TARCH,TARCH_fit,Alpha)\n",
    "    VaR_EGARCH = VaR_AR_Model (EGARCH,EGARCH_fit,Alpha)\n",
    "    VaR_AVGARCH = VaR_AR_Model (AVGARCH,AVGARCH_fit,Alpha)\n",
    "    VaR_FIGARCH = VaR_AR_Model (FIGARCH,FIGARCH_fit,Alpha)\n",
    "    return {'VaR_GARCH':VaR_GARCH, 'VaR_GJR_GARCH':VaR_GJR_GARCH, 'VaR_TARCH':VaR_TARCH, 'VaR_EGARCH':VaR_EGARCH, 'VaR_AVGARCH':VaR_AVGARCH, 'VaR_FIGARCH':VaR_FIGARCH}\n",
    "\n",
    "\n",
    "\n",
    "#Fitting of Transformed ANN-ARCH model and forecasting of the next volatility value\n",
    "def T_ANN_ARCH_Fit (Data,Database, Lag=1, LagSD=5, Timestep=10, Dropout=0.05, LearningRate=0.01, Epochs=10000, Alpha=0.005, DF=4, BatchSize=64):\n",
    "    #AR Models are fitted\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    #Database contaning AR models is generated\n",
    "    Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                     CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "    if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "    #Original explanatory and response variables are generated\n",
    "    XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "    #Data is normalized\n",
    "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "    #Data for fitting the transformer model is generated\n",
    "    XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "    #Model with transformer layer is defined\n",
    "    model = Transformer_Model(XData_AR_Norm_T.shape[1], XData_AR_Norm_T.shape[2], HeadsAttention=4, Dropout=Dropout, LearningRate=LearningRate)\n",
    "    model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, verbose=0, batch_size=BatchSize); tf.keras.backend.clear_session()\n",
    "    Forecast, Date_Forecast, TrainPrediction, ReturnForecast = T_ANN_ARCH_Forecast(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n",
    "    VaR = T_ANN_ARCH_VaR(Alpha, Data['DailyReturnsOld'], Forecast,DF)\n",
    "    return {'T_ANN_ARCH_model':model, 'Forecast_T_ANN_ARCH':Forecast, 'Date_Forecast':Date_Forecast, 'TrainPrediction': TrainPrediction, 'Scaler':Scaled_Norm, 'Forecast_GARCH':For_CV_GARCH, 'Forecast_GJR_GARCH':For_CV_GJR_GARCH, 'Forecast_TARCH':For_CV_TARCH, 'Forecast_EGARCH':For_CV_EGARCH, 'Forecast_AVGARCH':For_CV_AVGARCH, 'Forecast_FIGARCH':For_CV_FIGARCH, 'ReturnForecast':ReturnForecast, 'GARCH_fit': GARCH_Parameters, 'GJR_GARCH_fit':GJR_GARCH_Parameters, 'TARCH_fit':TARCH_Parameters, 'EGARCH_fit':EGARCH_Parameters, 'AVGARCH_fit':AVGARCH_Parameters, 'FIGARCH_fit':FIGARCH_Parameters, 'GARCH': GARCH, 'GJR_GARCH':GJR_GARCH, 'TARCH':TARCH, 'EGARCH':EGARCH, 'AVGARCH':AVGARCH, 'FIGARCH':FIGARCH, 'YData_Train':YData_AR_Norm_T, 'VaR': VaR}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTL-GARH model (Dropout=0)\n",
    "\n",
    "Model run for obtaining the training error of the MTL-GARCH model with dropout equal to 0. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.__future__ import reindexing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a high-level summary of what the code is doing:\n",
    "\n",
    "1. **Defining parameters and generating initial data**: The code first defines several parameters like the lag and the learning rate. It also generates an initial dataset based on the S&P 500 index (represented by \"^GSPC\") from Yahoo Finance.\n",
    "\n",
    "2. **Creating validation and results collections**: Next, it creates a dataset for validation and a DataFrame to store the results of the predictions.\n",
    "\n",
    "3. **Forecasting loop**: This is the main part of the code where it loops through each day in the `IndexEndDays` index. For each day, it:\n",
    "   \n",
    "   - Downloads the historical data up to that day.\n",
    "   \n",
    "   - Generates the input data for the models using the `DatabaseGeneration` function.\n",
    "   \n",
    "   - Fits the Transformed ANN-ARCH model and various other models using the `T_ANN_ARCH_Fit` function. This returns a dictionary of model results, including the fitted models and their volatility forecasts.\n",
    "   \n",
    "   - Computes the Value-at-Risk (VaR) of the different ARCH models using the `VaR_AR_Total` function.\n",
    "   \n",
    "   - Stores the results for that iteration in a dictionary called `IterResults`. This includes the forecasted date, the forecasted volatility from the various models, the true volatility, the VaR of the different models, etc.\n",
    "   \n",
    "   - Appends `IterResults` to the results collection DataFrame.\n",
    "\n",
    "4. **Saving results**: Finally, it saves the results collection to a CSV file after each iteration.\n",
    "\n",
    "In terms of prediction, it appears to be using a rolling-window approach where it uses the most recent 650 days of data to predict the next day's volatility. It does this for each day in the `IndexEndDays` index. The results are then collected and saved after each prediction. \n",
    "\n",
    "It also computes the VaR for each model in each iteration. VaR is a measure of the risk of loss for investments. It estimates how much a set of investments might lose, given normal market conditions, in a set time period such as a day.\n",
    "\n",
    "Overall, the goal seems to be to evaluate the performance of the Transformed ANN-ARCH model against various other models in predicting future volatility and assessing risk (VaR). The results are then stored for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 2014)\n"
     ]
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "\n",
    "    ResultsCollection.loc[len(ResultsCollection)] = IterResults\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('5_MTL_GARCH.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/418 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "  0%|          | 0/418 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"lstm\" (type LSTM).\n\nslice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,8], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n\nCall arguments received by layer \"lstm\" (type LSTM):\n  • inputs=tf.Tensor(shape=(None, 0, 8), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m Data \u001b[39m=\u001b[39m DatabaseGeneration(Database, Lag, LagSD)\n\u001b[1;32m     13\u001b[0m \u001b[39m#Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m T_ANN_ARCH_Model \u001b[39m=\u001b[39m T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n\u001b[1;32m     15\u001b[0m \u001b[39m#VaR of ARCH models is computed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m VaR_ARCH_Models\u001b[39m=\u001b[39mVaR_AR_Total(Alpha, T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGJR_GARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mTARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mEGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mAVGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mFIGARCH_fit\u001b[39m\u001b[39m'\u001b[39m],T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGJR_GARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mTARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mEGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mAVGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mFIGARCH\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 267\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Fit\u001b[0;34m(Data, Database, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF, BatchSize)\u001b[0m\n\u001b[1;32m    265\u001b[0m XData_AR_Norm_T, YData_AR_Norm_T\u001b[39m=\u001b[39m Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n\u001b[1;32m    266\u001b[0m \u001b[39m#Model with transformer layer is defined\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m model \u001b[39m=\u001b[39m Transformer_Model(XData_AR_Norm_T\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], XData_AR_Norm_T\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m], HeadsAttention\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, Dropout\u001b[39m=\u001b[39;49mDropout, LearningRate\u001b[39m=\u001b[39;49mLearningRate)\n\u001b[1;32m    268\u001b[0m model\u001b[39m.\u001b[39mfit(XData_AR_Norm_T, YData_AR_Norm_T, epochs\u001b[39m=\u001b[39mEpochs, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, batch_size\u001b[39m=\u001b[39mBatchSize); tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mclear_session()\n\u001b[1;32m    269\u001b[0m Forecast, Date_Forecast, TrainPrediction, ReturnForecast \u001b[39m=\u001b[39m T_ANN_ARCH_Forecast(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n",
      "Cell \u001b[0;32mIn[8], line 167\u001b[0m, in \u001b[0;36mTransformer_Model\u001b[0;34m(Shape1, Shape2, HeadsAttention, Dropout, LearningRate)\u001b[0m\n\u001b[1;32m    165\u001b[0m Input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(Shape1,Shape2), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[39m#LSTM is applied on top of the transformer\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m X \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mLSTM(units\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, dropout\u001b[39m=\u001b[39;49mDropout, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(Input)\n\u001b[1;32m    168\u001b[0m \u001b[39m#Tranformer architecture is implemented\u001b[39;00m\n\u001b[1;32m    169\u001b[0m transformer_block_1 \u001b[39m=\u001b[39m TransformerBlock(embed_dim\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, num_heads\u001b[39m=\u001b[39mHeadsAttention, ff_dim\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, rate\u001b[39m=\u001b[39mDropout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/keras/src/layers/rnn/base_rnn.py:556\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[1;32m    552\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/keras/src/backend.py:4981\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4970\u001b[0m input_ta \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m   4971\u001b[0m     ta\u001b[39m.\u001b[39munstack(input_)\n\u001b[1;32m   4972\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m go_backwards\n\u001b[1;32m   4973\u001b[0m     \u001b[39melse\u001b[39;00m ta\u001b[39m.\u001b[39munstack(reverse(input_, \u001b[39m0\u001b[39m))\n\u001b[1;32m   4974\u001b[0m     \u001b[39mfor\u001b[39;00m ta, input_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(input_ta, flatted_inputs)\n\u001b[1;32m   4975\u001b[0m )\n\u001b[1;32m   4977\u001b[0m \u001b[39m# Get the time(0) input and compute the output for that, the output will\u001b[39;00m\n\u001b[1;32m   4978\u001b[0m \u001b[39m# be used to determine the dtype of output tensor array. Don't read from\u001b[39;00m\n\u001b[1;32m   4979\u001b[0m \u001b[39m# input_ta due to TensorArray clear_after_read default to True.\u001b[39;00m\n\u001b[1;32m   4980\u001b[0m input_time_zero \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m-> 4981\u001b[0m     inputs, [inp[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m flatted_inputs]\n\u001b[1;32m   4982\u001b[0m )\n\u001b[1;32m   4983\u001b[0m \u001b[39m# output_time_zero is used to determine the cell output shape and its\u001b[39;00m\n\u001b[1;32m   4984\u001b[0m \u001b[39m# dtype.  the value is discarded.\u001b[39;00m\n\u001b[1;32m   4985\u001b[0m output_time_zero, _ \u001b[39m=\u001b[39m step_function(\n\u001b[1;32m   4986\u001b[0m     input_time_zero, \u001b[39mtuple\u001b[39m(initial_states) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(constants)\n\u001b[1;32m   4987\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"lstm\" (type LSTM).\n\nslice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,8], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n\nCall arguments received by layer \"lstm\" (type LSTM):\n  • inputs=tf.Tensor(shape=(None, 0, 8), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).resample('W-FRI').last().index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False).resample('W-FRI').last(), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False).resample('W-FRI').last()\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration(Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "\n",
    "    ResultsCollection.loc[len(ResultsCollection)] = IterResults\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('5_MTL_GARCH.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2014 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "  0%|          | 0/2014 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m Data \u001b[39m=\u001b[39m DatabaseGeneration (Database, Lag, LagSD)\n\u001b[1;32m     13\u001b[0m \u001b[39m#Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m T_ANN_ARCH_Model \u001b[39m=\u001b[39m T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n\u001b[1;32m     15\u001b[0m \u001b[39m#VaR of ARCH models is computed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m VaR_ARCH_Models\u001b[39m=\u001b[39mVaR_AR_Total(Alpha, T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGJR_GARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mTARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mEGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mAVGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mFIGARCH_fit\u001b[39m\u001b[39m'\u001b[39m],T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGJR_GARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mTARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mEGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mAVGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mFIGARCH\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 269\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Fit\u001b[0;34m(Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF, BatchSize)\u001b[0m\n\u001b[1;32m    267\u001b[0m model \u001b[39m=\u001b[39m Transformer_Model(XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], HeadsAttention\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, Dropout\u001b[39m=\u001b[39mDropout, LearningRate\u001b[39m=\u001b[39mLearningRate)\n\u001b[1;32m    268\u001b[0m model\u001b[39m.\u001b[39mfit(XData_AR_Norm_T, YData_AR_Norm_T, epochs\u001b[39m=\u001b[39mEpochs, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, batch_size\u001b[39m=\u001b[39mBatchSize); tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mclear_session()\n\u001b[0;32m--> 269\u001b[0m Forecast, Date_Forecast, TrainPrediction, ReturnForecast \u001b[39m=\u001b[39m T_ANN_ARCH_Forecast(Data, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n\u001b[1;32m    270\u001b[0m VaR \u001b[39m=\u001b[39m T_ANN_ARCH_VaR(Alpha, Data[\u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m], Forecast,DF)\n\u001b[1;32m    271\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mT_ANN_ARCH_model\u001b[39m\u001b[39m'\u001b[39m:model, \u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m:Forecast, \u001b[39m'\u001b[39m\u001b[39mDate_Forecast\u001b[39m\u001b[39m'\u001b[39m:Date_Forecast, \u001b[39m'\u001b[39m\u001b[39mTrainPrediction\u001b[39m\u001b[39m'\u001b[39m: TrainPrediction, \u001b[39m'\u001b[39m\u001b[39mScaler\u001b[39m\u001b[39m'\u001b[39m:Scaled_Norm, \u001b[39m'\u001b[39m\u001b[39mForecast_GARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_GARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_GJR_GARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_GJR_GARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_TARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_TARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_EGARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_EGARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_AVGARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_AVGARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_FIGARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_FIGARCH, \u001b[39m'\u001b[39m\u001b[39mReturnForecast\u001b[39m\u001b[39m'\u001b[39m:ReturnForecast, \u001b[39m'\u001b[39m\u001b[39mGARCH_fit\u001b[39m\u001b[39m'\u001b[39m: GARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mGJR_GARCH_fit\u001b[39m\u001b[39m'\u001b[39m:GJR_GARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mTARCH_fit\u001b[39m\u001b[39m'\u001b[39m:TARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mEGARCH_fit\u001b[39m\u001b[39m'\u001b[39m:EGARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mAVGARCH_fit\u001b[39m\u001b[39m'\u001b[39m:AVGARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mFIGARCH_fit\u001b[39m\u001b[39m'\u001b[39m:FIGARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mGARCH\u001b[39m\u001b[39m'\u001b[39m: GARCH, \u001b[39m'\u001b[39m\u001b[39mGJR_GARCH\u001b[39m\u001b[39m'\u001b[39m:GJR_GARCH, \u001b[39m'\u001b[39m\u001b[39mTARCH\u001b[39m\u001b[39m'\u001b[39m:TARCH, \u001b[39m'\u001b[39m\u001b[39mEGARCH\u001b[39m\u001b[39m'\u001b[39m:EGARCH, \u001b[39m'\u001b[39m\u001b[39mAVGARCH\u001b[39m\u001b[39m'\u001b[39m:AVGARCH, \u001b[39m'\u001b[39m\u001b[39mFIGARCH\u001b[39m\u001b[39m'\u001b[39m:FIGARCH, \u001b[39m'\u001b[39m\u001b[39mYData_Train\u001b[39m\u001b[39m'\u001b[39m:YData_AR_Norm_T, \u001b[39m'\u001b[39m\u001b[39mVaR\u001b[39m\u001b[39m'\u001b[39m: VaR}\n",
      "Cell \u001b[0;32mIn[5], line 213\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Forecast\u001b[0;34m(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH, Scaled_Norm, XData_AR, model)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mT_ANN_ARCH_Forecast\u001b[39m (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n\u001b[0;32m--> 213\u001b[0m     XDataForecast, ReturnForecast \u001b[39m=\u001b[39m DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n\u001b[1;32m    214\u001b[0m     XDataForecast \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([XData_AR,XDataForecast])\n\u001b[1;32m    215\u001b[0m     XDataForecastTotalScaled \u001b[39m=\u001b[39m Scaled_Norm\u001b[39m.\u001b[39mtransform(XDataForecast)\n",
      "Cell \u001b[0;32mIn[5], line 204\u001b[0m, in \u001b[0;36mDatabaseGenerationForecast_AR\u001b[0;34m(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDatabaseGenerationForecast_AR\u001b[39m (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n\u001b[0;32m--> 204\u001b[0m     Data_Forecast\u001b[39m=\u001b[39mDatabaseGenerationForecast(Database, Lag, LagSD)\u001b[39m.\u001b[39miloc[(\u001b[39m-\u001b[39mLagSD\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[1;32m    205\u001b[0m     Index_Forecast\u001b[39m=\u001b[39mDatabaseGenerationForecast(Database, Lag, LagSD)\u001b[39m.\u001b[39mindex[(\u001b[39m-\u001b[39mLagSD\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[1;32m    206\u001b[0m     XDataForecast\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mSD\u001b[39m\u001b[39m'\u001b[39m: Data_Forecast[\u001b[39m'\u001b[39m\u001b[39mSD\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m: Data_Forecast[\u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    207\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mCV_GARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_GARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_GJR_GARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_GJR_GARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_TARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_TARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \n\u001b[1;32m    208\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mCV_EGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_EGARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_AVGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_AVGARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_FIGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_FIGARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m}\n",
      "Cell \u001b[0;32mIn[5], line 194\u001b[0m, in \u001b[0;36mDatabaseGenerationForecast\u001b[0;34m(Database, Lag, LagSD)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDatabaseGenerationForecast\u001b[39m (Database, Lag, LagSD):\n\u001b[0;32m--> 194\u001b[0m     DailyReturns, Index \u001b[39m=\u001b[39m ReturnCalculation(Database,Lag)\n\u001b[1;32m    195\u001b[0m     DailyReturnsOld \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39mnan, \u001b[39m1\u001b[39m),DailyReturns[\u001b[39m0\u001b[39m:(DailyReturns\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[1;32m    196\u001b[0m     SD \u001b[39m=\u001b[39m SDCalculation (DailyReturns, LagSD)\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mReturnCalculation\u001b[0;34m(Database, lag)\u001b[0m\n\u001b[1;32m      3\u001b[0m dimension\u001b[39m=\u001b[39mDatabase\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m];dif\u001b[39m=\u001b[39mlag;Out\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros([dimension\u001b[39m-\u001b[39mdif])\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(dimension\u001b[39m-\u001b[39mdif):\n\u001b[0;32m----> 5\u001b[0m     Out[i]\u001b[39m=\u001b[39m(np\u001b[39m.\u001b[39mlog(Database[\u001b[39m'\u001b[39;49m\u001b[39mClose\u001b[39;49m\u001b[39m'\u001b[39;49m][i\u001b[39m+\u001b[39mdif])\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(Database[\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m][i]))\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39mnan, dif),Out), Database\u001b[39m.\u001b[39mindex\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/thesis_2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Close'"
     ]
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; \n",
    "IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=100; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=1000), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "\n",
    "    ResultsCollection.loc[len(ResultsCollection)] = IterResults\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('5_MTL_GARCH.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.__future__ import reindexingcn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTL-GARH model (Dropout=0.05)\n",
    "\n",
    "Model run for obtaining the training error of the MTL-GARCH model with dropout equal to 0.05. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.05; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('2. Drop005/5_MTL_GARCH.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTL-GARH model (Dropout=0.10)\n",
    "\n",
    "Model run for obtaining the training error of the MTL-GARCH model with dropout equal to 0.10. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.10; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('3. Drop010/5_MTL_GARCH.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-GARH model (Dropout=0.15)\n",
    "\n",
    "Model run for obtaining the training error of the MTL-GARCH model with dropout equal to 0.15. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.15; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('4. Drop015/5_MTL_GARCH.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
